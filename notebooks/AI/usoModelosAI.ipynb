{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# how can i import a function to measure balanced accuracy?\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../python')\n",
    "import splitDataset\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data1= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil1.csv', sep=\";\")\n",
    "data2= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil2.csv', sep=\";\")\n",
    "data3= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil3.csv', sep=\";\")\n",
    "\n",
    "data1 = data1[data1.bestSubmit != 100.0]\n",
    "data2 = data2[data2.bestSubmit != 100.0]\n",
    "data3 = data3[data3.bestSubmit != 100.0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows X_train1 dataset:  3295\n",
      "Number of rows X_test1 dataset:  1615\n",
      "Number of rows X_train2 dataset:  2551\n",
      "Number of rows X_test2 dataset:  1286\n",
      "Number of rows X_train3 dataset:  2079\n",
      "Number of rows X_test3 dataset:  971\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = splitDataset.splitDataset(data1)\n",
    "X_train2, X_test2, y_train2, y_test2 = splitDataset.splitDataset(data2)\n",
    "X_train3, X_test3, y_train3, y_test3 = splitDataset.splitDataset(data3)\n",
    "\n",
    "# show the number of rows of X_train1\n",
    "print(\"Number of rows X_train1 dataset: \", X_train1.shape[0])\n",
    "print(\"Number of rows X_test1 dataset: \", X_test1.shape[0])\n",
    "\n",
    "print(\"Number of rows X_train2 dataset: \", X_train2.shape[0])\n",
    "print(\"Number of rows X_test2 dataset: \", X_test2.shape[0])\n",
    "\n",
    "print(\"Number of rows X_train3 dataset: \", X_train3.shape[0])\n",
    "print(\"Number of rows X_test3 dataset: \", X_test3.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model_lr_1 = joblib.load('./models2/lr_1.pkl')\n",
    "model_lr_2 = joblib.load('./models2/lr_2.pkl')\n",
    "model_lr_3 = joblib.load('./models2/lr_3.pkl')\n",
    "\n",
    "model_rfc_1 = joblib.load('./models2/rfc_1.pkl')\n",
    "model_rfc_2 = joblib.load('./models2/rfc_2.pkl')\n",
    "model_rfc_3 = joblib.load('./models2/rfc_3.pkl')\n",
    "\n",
    "model_tree_1 = joblib.load('./models2/tree_1.pkl')\n",
    "model_tree_2 = joblib.load('./models2/tree_2.pkl')\n",
    "model_tree_3 = joblib.load('./models2/tree_3.pkl')\n",
    "\n",
    "model_svc_1 = joblib.load('./models2/svc_1.pkl')\n",
    "model_svc_2 = joblib.load('./models2/svc_2.pkl')\n",
    "model_svc_3 = joblib.load('./models2/svc_3.pkl')\n",
    "\n",
    "model_knn_1 = joblib.load('./models2/knn_1.pkl')\n",
    "model_knn_2 = joblib.load('./models2/knn_2.pkl')\n",
    "model_knn_3 = joblib.load('./models2/knn_3.pkl')\n",
    "\n",
    "model_mlp_1 = joblib.load('./models2/mlp_1.pkl')\n",
    "model_mlp_2 = joblib.load('./models2/mlp_2.pkl')\n",
    "model_mlp_3 = joblib.load('./models2/mlp_3.pkl')\n",
    "\n",
    "model_sgd_1 = joblib.load('./models2/sgd_1.pkl')\n",
    "model_sgd_2 = joblib.load('./models2/sgd_2.pkl')\n",
    "model_sgd_3 = joblib.load('./models2/sgd_3.pkl')\n",
    "\n",
    "model_adaboost_1 = joblib.load('./models2/adaboost_1.pkl')\n",
    "model_adaboost_2 = joblib.load('./models2/adaboost_2.pkl')\n",
    "model_adaboost_3 = joblib.load('./models2/adaboost_3.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random forest classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR RFC\n",
      "\n",
      "-----Balanced accuracy-----\n",
      "percentile 1 : 0.7536491677336747\n",
      "percentile 2 : 0.7839334907516726\n",
      "percentile 3 : 0.7978226928690222\n",
      "\n",
      "-----F1 score-----\n",
      "percentile 1 : 0.8701298701298701\n",
      "percentile 2 : 0.8356687898089172\n",
      "percentile 3 : 0.8108108108108109\n",
      "\n",
      "-----Cohen kappa score-----\n",
      "percentile 1 : 0.5508898776418243\n",
      "percentile 2 : 0.5818845364774199\n",
      "percentile 3 : 0.5960246907290897\n",
      "-----Confusion matrix-----\n",
      "percentile 1 : [[ 310  240]\n",
      " [  60 1005]]\n",
      "percentile 2 : [[372 188]\n",
      " [ 70 656]]\n",
      "percentile 3 : [[355 128]\n",
      " [ 68 420]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_rfc_1 = model_rfc_1.predict(X_test1)\n",
    "y_pred_rfc_2 = model_rfc_2.predict(X_test2)\n",
    "y_pred_rfc_3 = model_rfc_3.predict(X_test3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('RESULTS FOR RFC')\n",
    "print()\n",
    "print('-----Balanced accuracy-----')\n",
    "print('percentile 1 : ' + str(balanced_accuracy_score(y_test1, y_pred_rfc_1)))\n",
    "print('percentile 2 : ' + str(balanced_accuracy_score(y_test2, y_pred_rfc_2)))\n",
    "print('percentile 3 : ' + str(balanced_accuracy_score(y_test3, y_pred_rfc_3)))\n",
    "print()\n",
    "print('-----F1 score-----')\n",
    "print('percentile 1 : ' + str(f1_score(y_test1, y_pred_rfc_1)))\n",
    "print('percentile 2 : ' + str(f1_score(y_test2, y_pred_rfc_2)))\n",
    "print('percentile 3 : ' + str(f1_score(y_test3, y_pred_rfc_3)))\n",
    "print()\n",
    "print('-----Cohen kappa score-----')\n",
    "print('percentile 1 : ' + str(cohen_kappa_score(y_test1, y_pred_rfc_1)))\n",
    "print('percentile 2 : ' + str(cohen_kappa_score(y_test2, y_pred_rfc_2)))\n",
    "print('percentile 3 : ' + str(cohen_kappa_score(y_test3, y_pred_rfc_3)))\n",
    "print('-----Confusion matrix-----')\n",
    "print('percentile 1 : ' + str(confusion_matrix(y_test1, y_pred_rfc_1)))\n",
    "print('percentile 2 : ' + str(confusion_matrix(y_test2, y_pred_rfc_2)))\n",
    "print('percentile 3 : ' + str(confusion_matrix(y_test3, y_pred_rfc_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stochastic gradient descent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR SGD\n",
      "\n",
      "-----Balanced accuracy-----\n",
      "percentile 1 : 0.7241912078531797\n",
      "percentile 2 : 0.7272087760724124\n",
      "percentile 3 : 0.7140460068560568\n",
      "\n",
      "-----F1 score-----\n",
      "percentile 1 : 0.84108012394865\n",
      "percentile 2 : 0.7438494934876989\n",
      "percentile 3 : 0.7488667271078876\n",
      "\n",
      "-----Cohen kappa score-----\n",
      "percentile 1 : 0.47533855474564846\n",
      "percentile 2 : 0.44801513204161314\n",
      "percentile 3 : 0.4286694436596832\n",
      "\n",
      "-----Confusion matrix-----\n",
      "percentile 1 : [[306 244]\n",
      " [115 950]]\n",
      "percentile 2 : [[418 142]\n",
      " [212 514]]\n",
      "percentile 3 : [[281 202]\n",
      " [ 75 413]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_sgd_1 = model_sgd_1.predict(X_test1)\n",
    "y_pred_sgd_2 = model_sgd_2.predict(X_test2)\n",
    "y_pred_sgd_3 = model_sgd_3.predict(X_test3)\n",
    "\n",
    "print('RESULTS FOR SGD')\n",
    "print()\n",
    "print('-----Balanced accuracy-----')\n",
    "print('percentile 1 : ' + str(balanced_accuracy_score(y_test1, y_pred_sgd_1)))\n",
    "print('percentile 2 : ' + str(balanced_accuracy_score(y_test2, y_pred_sgd_2)))\n",
    "print('percentile 3 : ' + str(balanced_accuracy_score(y_test3, y_pred_sgd_3)))\n",
    "print()\n",
    "print('-----F1 score-----')\n",
    "print('percentile 1 : ' + str(f1_score(y_test1, y_pred_sgd_1)))\n",
    "print('percentile 2 : ' + str(f1_score(y_test2, y_pred_sgd_2)))\n",
    "print('percentile 3 : ' + str(f1_score(y_test3, y_pred_sgd_3)))\n",
    "print()\n",
    "print('-----Cohen kappa score-----')\n",
    "print('percentile 1 : ' + str(cohen_kappa_score(y_test1, y_pred_sgd_1)))\n",
    "print('percentile 2 : ' + str(cohen_kappa_score(y_test2, y_pred_sgd_2)))\n",
    "print('percentile 3 : ' + str(cohen_kappa_score(y_test3, y_pred_sgd_3)))\n",
    "print()\n",
    "print('-----Confusion matrix-----')\n",
    "print('percentile 1 : ' + str(confusion_matrix(y_test1, y_pred_sgd_1)))\n",
    "print('percentile 2 : ' + str(confusion_matrix(y_test2, y_pred_sgd_2)))\n",
    "print('percentile 3 : ' + str(confusion_matrix(y_test3, y_pred_sgd_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-layer perceptron classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR MLP\n",
      "\n",
      "-----Balanced accuracy-----\n",
      "percentile 1 : 0.7420614596670935\n",
      "percentile 2 : 0.7785050177095632\n",
      "percentile 3 : 0.7976105624002987\n",
      "\n",
      "-----F1 score-----\n",
      "percentile 1 : 0.8608695652173913\n",
      "percentile 2 : 0.8055363321799308\n",
      "percentile 3 : 0.8178438661710038\n",
      "\n",
      "-----Cohen kappa score-----\n",
      "percentile 1 : 0.523225241016652\n",
      "percentile 2 : 0.5562151687007695\n",
      "percentile 3 : 0.5958531178064506\n",
      "\n",
      "-----Confusion matrix-----\n",
      "percentile 1 : [[305 245]\n",
      " [ 75 990]]\n",
      "percentile 2 : [[423 137]\n",
      " [144 582]]\n",
      "percentile 3 : [[335 148]\n",
      " [ 48 440]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_mlp_1 = model_mlp_1.predict(X_test1)\n",
    "y_pred_mlp_2 = model_mlp_2.predict(X_test2)\n",
    "y_pred_mlp_3 = model_mlp_3.predict(X_test3)\n",
    "\n",
    "print('RESULTS FOR MLP')\n",
    "print()\n",
    "print('-----Balanced accuracy-----')\n",
    "print('percentile 1 : ' + str(balanced_accuracy_score(y_test1, y_pred_mlp_1)))\n",
    "print('percentile 2 : ' + str(balanced_accuracy_score(y_test2, y_pred_mlp_2)))\n",
    "print('percentile 3 : ' + str(balanced_accuracy_score(y_test3, y_pred_mlp_3)))\n",
    "print()\n",
    "print('-----F1 score-----')\n",
    "print('percentile 1 : ' + str(f1_score(y_test1, y_pred_mlp_1)))\n",
    "print('percentile 2 : ' + str(f1_score(y_test2, y_pred_mlp_2)))\n",
    "print('percentile 3 : ' + str(f1_score(y_test3, y_pred_mlp_3)))\n",
    "print()\n",
    "print('-----Cohen kappa score-----')\n",
    "print('percentile 1 : ' + str(cohen_kappa_score(y_test1, y_pred_mlp_1)))\n",
    "print('percentile 2 : ' + str(cohen_kappa_score(y_test2, y_pred_mlp_2)))\n",
    "print('percentile 3 : ' + str(cohen_kappa_score(y_test3, y_pred_mlp_3)))\n",
    "print()\n",
    "print('-----Confusion matrix-----')\n",
    "print('percentile 1 : ' + str(confusion_matrix(y_test1, y_pred_mlp_1)))\n",
    "print('percentile 2 : ' + str(confusion_matrix(y_test2, y_pred_mlp_2)))\n",
    "print('percentile 3 : ' + str(confusion_matrix(y_test3, y_pred_mlp_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AdaBoost classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR ADABOOST\n",
      "\n",
      "-----Balanced accuracy-----\n",
      "percentile 1 : 0.7697737942808365\n",
      "percentile 2 : 0.7764339826839827\n",
      "percentile 3 : 0.8010343481654957\n",
      "\n",
      "-----F1 score-----\n",
      "percentile 1 : 0.8721340388007055\n",
      "percentile 2 : 0.8288633461047253\n",
      "percentile 3 : 0.8094768015794668\n",
      "\n",
      "-----Cohen kappa score-----\n",
      "percentile 1 : 0.5743936461201531\n",
      "percentile 2 : 0.5660416320404535\n",
      "percentile 3 : 0.6023050657544304\n",
      "\n",
      "-----Confusion matrix-----\n",
      "percentile 1 : [[336 214]\n",
      " [ 76 989]]\n",
      "percentile 2 : [[369 191]\n",
      " [ 77 649]]\n",
      "percentile 3 : [[368 115]\n",
      " [ 78 410]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_adaboost_1 = model_adaboost_1.predict(X_test1)\n",
    "y_pred_adaboost_2 = model_adaboost_2.predict(X_test2)\n",
    "y_pred_adaboost_3 = model_adaboost_3.predict(X_test3)\n",
    "\n",
    "y_pred_adaboost_1 = y_pred_adaboost_1.astype(int)\n",
    "y_pred_adaboost_3 = y_pred_adaboost_3.astype(int)\n",
    "\n",
    "print('RESULTS FOR ADABOOST')\n",
    "print()\n",
    "print('-----Balanced accuracy-----')\n",
    "print('percentile 1 : ' + str(balanced_accuracy_score(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(balanced_accuracy_score(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(balanced_accuracy_score(y_test3, y_pred_adaboost_3)))\n",
    "print()\n",
    "print('-----F1 score-----')\n",
    "print('percentile 1 : ' + str(f1_score(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(f1_score(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(f1_score(y_test3, y_pred_adaboost_3)))\n",
    "print()\n",
    "print('-----Cohen kappa score-----')\n",
    "print('percentile 1 : ' + str(cohen_kappa_score(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(cohen_kappa_score(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(cohen_kappa_score(y_test3, y_pred_adaboost_3)))\n",
    "print()\n",
    "print('-----Confusion matrix-----')\n",
    "print('percentile 1 : ' + str(confusion_matrix(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(confusion_matrix(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(confusion_matrix(y_test3, y_pred_adaboost_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-nearest neighbors classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR KNN\n",
      "\n",
      "-----Balanced accuracy-----\n",
      "percentile 1 : 0.6754588134869824\n",
      "percentile 2 : 0.6930416174734357\n",
      "percentile 3 : 0.7409823337745647\n",
      "\n",
      "-----F1 score-----\n",
      "percentile 1 : 0.8292890591741165\n",
      "percentile 2 : 0.7795992714025501\n",
      "percentile 3 : 0.766076421248835\n",
      "\n",
      "-----Cohen kappa score-----\n",
      "percentile 1 : 0.3883009119545864\n",
      "percentile 2 : 0.402090297790586\n",
      "percentile 3 : 0.4824610392781835\n",
      "\n",
      "-----Confusion matrix-----\n",
      "percentile 1 : [[240 310]\n",
      " [ 91 974]]\n",
      "percentile 2 : [[281 279]\n",
      " [ 84 642]]\n",
      "percentile 3 : [[309 174]\n",
      " [ 77 411]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_knn_1 = model_knn_1.predict(X_test1)\n",
    "y_pred_knn_2 = model_knn_2.predict(X_test2)\n",
    "y_pred_knn_3 = model_knn_3.predict(X_test3)\n",
    "\n",
    "\n",
    "print('RESULTS FOR KNN')\n",
    "print()\n",
    "print('-----Balanced accuracy-----')\n",
    "print('percentile 1 : ' + str(balanced_accuracy_score(y_test1, y_pred_knn_1)))\n",
    "print('percentile 2 : ' + str(balanced_accuracy_score(y_test2, y_pred_knn_2)))\n",
    "print('percentile 3 : ' + str(balanced_accuracy_score(y_test3, y_pred_knn_3)))\n",
    "print()\n",
    "print('-----F1 score-----')\n",
    "print('percentile 1 : ' + str(f1_score(y_test1, y_pred_knn_1)))\n",
    "print('percentile 2 : ' + str(f1_score(y_test2, y_pred_knn_2)))\n",
    "print('percentile 3 : ' + str(f1_score(y_test3, y_pred_knn_3)))\n",
    "print()\n",
    "print('-----Cohen kappa score-----')\n",
    "print('percentile 1 : ' + str(cohen_kappa_score(y_test1, y_pred_knn_1)))\n",
    "print('percentile 2 : ' + str(cohen_kappa_score(y_test2, y_pred_knn_2)))\n",
    "print('percentile 3 : ' + str(cohen_kappa_score(y_test3, y_pred_knn_3)))\n",
    "print()\n",
    "print('-----Confusion matrix-----')\n",
    "print('percentile 1 : ' + str(confusion_matrix(y_test1, y_pred_knn_1)))\n",
    "print('percentile 2 : ' + str(confusion_matrix(y_test2, y_pred_knn_2)))\n",
    "print('percentile 3 : ' + str(confusion_matrix(y_test3, y_pred_knn_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision tree classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR DECISION TREE\n",
      "\n",
      "-----Balanced accuracy-----\n",
      "percentile 1 : 0.724950917626974\n",
      "percentile 2 : 0.7526834907516726\n",
      "percentile 3 : 0.7608525947798934\n",
      "\n",
      "-----F1 score-----\n",
      "percentile 1 : 0.8381631743201069\n",
      "percentile 2 : 0.8174454828660436\n",
      "percentile 3 : 0.7716535433070867\n",
      "\n",
      "-----Cohen kappa score-----\n",
      "percentile 1 : 0.473419233723014\n",
      "percentile 2 : 0.5216611951932281\n",
      "percentile 3 : 0.5219266226379858\n",
      "\n",
      "-----Confusion matrix-----\n",
      "percentile 1 : [[312 238]\n",
      " [125 940]]\n",
      "percentile 2 : [[337 223]\n",
      " [ 70 656]]\n",
      "percentile 3 : [[347 136]\n",
      " [ 96 392]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_tree_1 = model_tree_1.predict(X_test1)\n",
    "y_pred_tree_2 = model_tree_2.predict(X_test2)\n",
    "y_pred_tree_3 = model_tree_3.predict(X_test3)\n",
    "\n",
    "print('RESULTS FOR DECISION TREE')\n",
    "print()\n",
    "print('-----Balanced accuracy-----')\n",
    "print('percentile 1 : ' + str(balanced_accuracy_score(y_test1, y_pred_tree_1)))\n",
    "print('percentile 2 : ' + str(balanced_accuracy_score(y_test2, y_pred_tree_2)))\n",
    "print('percentile 3 : ' + str(balanced_accuracy_score(y_test3, y_pred_tree_3)))\n",
    "print()\n",
    "print('-----F1 score-----')\n",
    "print('percentile 1 : ' + str(f1_score(y_test1, y_pred_tree_1)))\n",
    "print('percentile 2 : ' + str(f1_score(y_test2, y_pred_tree_2)))\n",
    "print('percentile 3 : ' + str(f1_score(y_test3, y_pred_tree_3)))\n",
    "print()\n",
    "print('-----Cohen kappa score-----')\n",
    "print('percentile 1 : ' + str(cohen_kappa_score(y_test1, y_pred_tree_1)))\n",
    "print('percentile 2 : ' + str(cohen_kappa_score(y_test2, y_pred_tree_2)))\n",
    "print('percentile 3 : ' + str(cohen_kappa_score(y_test3, y_pred_tree_3)))\n",
    "print()\n",
    "print('-----Confusion matrix-----')\n",
    "print('percentile 1 : ' + str(confusion_matrix(y_test1, y_pred_tree_1)))\n",
    "print('percentile 2 : ' + str(confusion_matrix(y_test2, y_pred_tree_2)))\n",
    "print('percentile 3 : ' + str(confusion_matrix(y_test3, y_pred_tree_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Support vector machine classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR SVC\n",
      "\n",
      "-----Balanced accuracy-----\n",
      "percentile 1 : 0.7015023474178403\n",
      "percentile 2 : 0.7312303227075954\n",
      "percentile 3 : 0.7451019074771748\n",
      "\n",
      "-----F1 score-----\n",
      "percentile 1 : 0.842330762639246\n",
      "percentile 2 : 0.800747198007472\n",
      "percentile 3 : 0.7698042870456664\n",
      "\n",
      "-----Cohen kappa score-----\n",
      "percentile 1 : 0.4427065751472188\n",
      "percentile 2 : 0.4774720781760281\n",
      "percentile 3 : 0.4907086721183719\n",
      "\n",
      "-----Confusion matrix-----\n",
      "percentile 1 : [[264 286]\n",
      " [ 82 983]]\n",
      "percentile 2 : [[323 237]\n",
      " [ 83 643]]\n",
      "percentile 3 : [[311 172]\n",
      " [ 75 413]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_svc_1 = model_svc_1.predict(X_test1)\n",
    "y_pred_svc_2 = model_svc_2.predict(X_test2)\n",
    "y_pred_svc_3 = model_svc_3.predict(X_test3)\n",
    "\n",
    "print('RESULTS FOR SVC')\n",
    "print()\n",
    "print('-----Balanced accuracy-----')\n",
    "print('percentile 1 : ' + str(balanced_accuracy_score(y_test1, y_pred_svc_1)))\n",
    "print('percentile 2 : ' + str(balanced_accuracy_score(y_test2, y_pred_svc_2)))\n",
    "print('percentile 3 : ' + str(balanced_accuracy_score(y_test3, y_pred_svc_3)))\n",
    "print()\n",
    "print('-----F1 score-----')\n",
    "print('percentile 1 : ' + str(f1_score(y_test1, y_pred_svc_1)))\n",
    "print('percentile 2 : ' + str(f1_score(y_test2, y_pred_svc_2)))\n",
    "print('percentile 3 : ' + str(f1_score(y_test3, y_pred_svc_3)))\n",
    "print()\n",
    "print('-----Cohen kappa score-----')\n",
    "print('percentile 1 : ' + str(cohen_kappa_score(y_test1, y_pred_svc_1)))\n",
    "print('percentile 2 : ' + str(cohen_kappa_score(y_test2, y_pred_svc_2)))\n",
    "print('percentile 3 : ' + str(cohen_kappa_score(y_test3, y_pred_svc_3)))\n",
    "print()\n",
    "print('-----Confusion matrix-----')\n",
    "print('percentile 1 : ' + str(confusion_matrix(y_test1, y_pred_svc_1)))\n",
    "print('percentile 2 : ' + str(confusion_matrix(y_test2, y_pred_svc_2)))\n",
    "print('percentile 3 : ' + str(confusion_matrix(y_test3, y_pred_svc_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SHAP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Your Shapash application run on http://DESKTOP-O96AF7S:8050/\n",
      "INFO:root:Use the method .kill() to down your app.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://0.0.0.0:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dash.dash:Dash is running on http://0.0.0.0:8050/\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SmartExplainer' object has no attribute 'correlation_analysis'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 17\u001B[0m\n\u001B[0;32m      8\u001B[0m xpl\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[0;32m      9\u001B[0m     x\u001B[38;5;241m=\u001B[39mX_test3,\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m#y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;66;03m#additional_features_dict=features_dict_additional, # Optional: dict additional data\u001B[39;00m\n\u001B[0;32m     14\u001B[0m )\n\u001B[0;32m     15\u001B[0m app \u001B[38;5;241m=\u001B[39m xpl\u001B[38;5;241m.\u001B[39mrun_app()\n\u001B[1;32m---> 17\u001B[0m \u001B[43mxpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorrelation_analysis\u001B[49m()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'SmartExplainer' object has no attribute 'correlation_analysis'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'shapash.webapp.smart_app'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "from shapash import SmartExplainer\n",
    "xpl = SmartExplainer(\n",
    "    model=model_tree_3,\n",
    "    #features_dict=house_dict,  # Optional parameter\n",
    "    #preprocessing=encoder, # Optional: compile step can use inverse_transform method\n",
    "    #postprocessing=postprocess, # Optional: see tutorial postprocessing\n",
    ")\n",
    "xpl.compile(\n",
    "    x=X_test3,\n",
    "    #y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\n",
    "    y_target=y_test3,  # Optional: allows to display True Values vs Predicted Values\n",
    "    #additional_data=X_additional, # Optional: additional dataset of features for Webapp\n",
    "    #additional_features_dict=features_dict_additional, # Optional: dict additional data\n",
    ")\n",
    "app = xpl.run_app()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:papermill:Input Notebook:  C:\\Users\\alvar\\anaconda3\\envs\\xAI\\Lib\\site-packages\\shapash\\report\\base_report.ipynb\n",
      "INFO:papermill:Output Notebook: C:\\Users\\alvar\\AppData\\Local\\Temp\\tmpd_34grur\\base_report.ipynb\n",
      "WARNING:papermill.translators:Black is not installed, parameters wont be formatted\n"
     ]
    },
    {
     "data": {
      "text/plain": "Executing:   0%|          | 0/15 [00:00<?, ?cell/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6fa1c8bf5d74551b0addb3c26328df0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:papermill:Executing notebook with kernel: python3\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u25c4' in position 1779304: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeEncodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mxpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_report\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mreports/report.html\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproject_info_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mreportsyml/project_info.yml\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_train3\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_train3\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_test\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_test3\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtitle_story\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHouse prices report\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtitle_description\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[38;5;124;43mThis document is a data science report of the kaggle house prices tutorial project.\u001B[39;49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;43m        It was generated using the Shapash library.\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mname\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMSE\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpath\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msklearn.metrics.mean_squared_error\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\explainer\\smart_explainer.py:1286\u001B[0m, in \u001B[0;36mSmartExplainer.generate_report\u001B[1;34m(self, output_file, project_info_file, x_train, y_train, y_test, title_story, title_description, metrics, working_dir, notebook_path, kernel_name)\u001B[0m\n\u001B[0;32m   1284\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rm_working_dir:\n\u001B[0;32m   1285\u001B[0m     shutil\u001B[38;5;241m.\u001B[39mrmtree(working_dir)\n\u001B[1;32m-> 1286\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\explainer\\smart_explainer.py:1278\u001B[0m, in \u001B[0;36mSmartExplainer.generate_report\u001B[1;34m(self, output_file, project_info_file, x_train, y_train, y_test, title_story, title_description, metrics, working_dir, notebook_path, kernel_name)\u001B[0m\n\u001B[0;32m   1262\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1263\u001B[0m     execute_report(\n\u001B[0;32m   1264\u001B[0m         working_dir\u001B[38;5;241m=\u001B[39mworking_dir,\n\u001B[0;32m   1265\u001B[0m         explainer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1276\u001B[0m         kernel_name\u001B[38;5;241m=\u001B[39mkernel_name\n\u001B[0;32m   1277\u001B[0m     )\n\u001B[1;32m-> 1278\u001B[0m     \u001B[43mexport_and_save_report\u001B[49m\u001B[43m(\u001B[49m\u001B[43mworking_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworking_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m rm_working_dir:\n\u001B[0;32m   1281\u001B[0m         shutil\u001B[38;5;241m.\u001B[39mrmtree(working_dir)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\report\\generation.py:94\u001B[0m, in \u001B[0;36mexport_and_save_report\u001B[1;34m(working_dir, output_file)\u001B[0m\n\u001B[0;32m     91\u001B[0m (body, resources) \u001B[38;5;241m=\u001B[39m exporter\u001B[38;5;241m.\u001B[39mfrom_filename(filename\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(working_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbase_report.ipynb\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(output_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m---> 94\u001B[0m     \u001B[43mfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\encodings\\cp1252.py:19\u001B[0m, in \u001B[0;36mIncrementalEncoder.encode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mencode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m---> 19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcodecs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcharmap_encode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43mencoding_table\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mUnicodeEncodeError\u001B[0m: 'charmap' codec can't encode character '\\u25c4' in position 1779304: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "xpl.generate_report(\n",
    "    output_file='reports/report.html',\n",
    "    project_info_file='reportsyml/project_info.yml',\n",
    "    x_train=X_train3,\n",
    "    y_train=y_train3,\n",
    "    y_test=y_test3,\n",
    "    title_story=\"House prices report\",\n",
    "    title_description=\"\"\"This document is a data science report of the kaggle house prices tutorial project.\n",
    "        It was generated using the Shapash library.\"\"\",\n",
    "    metrics=[{'name': 'MSE', 'path': 'sklearn.metrics.mean_squared_error'}],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Percentil 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://0.0.0.0:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Your Shapash application run on http://DESKTOP-O96AF7S:8050/\n",
      "INFO:dash.dash:Dash is running on http://0.0.0.0:8050/\n",
      "\n",
      "INFO:root:Use the method .kill() to down your app.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'shapash.webapp.smart_app'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "xpl = SmartExplainer(\n",
    "    model=model_rfc_1,\n",
    "    #features_dict=house_dict,  # Optional parameter\n",
    "    #preprocessing=encoder, # Optional: compile step can use inverse_transform method\n",
    "    #postprocessing=postprocess, # Optional: see tutorial postprocessing\n",
    ")\n",
    "xpl.compile(\n",
    "    x=X_test1,\n",
    "    #y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\n",
    "    y_target=y_test1,  # Optional: allows to display True Values vs Predicted Values\n",
    "    #additional_data=X_additional, # Optional: additional dataset of features for Webapp\n",
    "    #additional_features_dict=features_dict_additional, # Optional: dict additional data\n",
    ")\n",
    "app = xpl.run_app()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Percentil 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xpl = SmartExplainer(\n",
    "    model=model_rfc_2,\n",
    "    #features_dict=house_dict,  # Optional parameter\n",
    "    #preprocessing=encoder, # Optional: compile step can use inverse_transform method\n",
    "    #postprocessing=postprocess, # Optional: see tutorial postprocessing\n",
    ")\n",
    "xpl.compile(\n",
    "    x=X_test2,\n",
    "    #y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\n",
    "    y_target=y_test2,  # Optional: allows to display True Values vs Predicted Values\n",
    "    #additional_data=X_additional, # Optional: additional dataset of features for Webapp\n",
    "    #additional_features_dict=features_dict_additional, # Optional: dict additional data\n",
    ")\n",
    "app = xpl.run_app()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Percentil 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://0.0.0.0:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Your Shapash application run on http://DESKTOP-O96AF7S:8050/\n",
      "INFO:root:Use the method .kill() to down your app.\n",
      "INFO:dash.dash:Dash is running on http://0.0.0.0:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'shapash.webapp.smart_app'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "xpl = SmartExplainer(\n",
    "    model=model_rfc_3,\n",
    "    #features_dict=house_dict,  # Optional parameter\n",
    "    #preprocessing=encoder, # Optional: compile step can use inverse_transform method\n",
    "    #postprocessing=postprocess, # Optional: see tutorial postprocessing\n",
    ")\n",
    "xpl.compile(\n",
    "    x=X_test3,\n",
    "    #y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\n",
    "    y_target=y_test3,  # Optional: allows to display True Values vs Predicted Values\n",
    "    #additional_data=X_additional, # Optional: additional dataset of features for Webapp\n",
    "    #additional_features_dict=features_dict_additional, # Optional: dict additional data\n",
    ")\n",
    "app = xpl.run_app()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The passed model is not callable and cannot be analyzed directly with the given masker! Model: SGDClassifier(loss='modified_huber', max_iter=500, penalty='l1',\n              random_state=42)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m xpl \u001B[38;5;241m=\u001B[39m \u001B[43mSmartExplainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_sgd_3\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#features_dict=house_dict,  # Optional parameter\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#preprocessing=encoder, # Optional: compile step can use inverse_transform method\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#postprocessing=postprocess, # Optional: see tutorial postprocessing\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m xpl\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[0;32m      8\u001B[0m     x\u001B[38;5;241m=\u001B[39mX_test3,\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m#y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m#additional_features_dict=features_dict_additional, # Optional: dict additional data\u001B[39;00m\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     14\u001B[0m app \u001B[38;5;241m=\u001B[39m xpl\u001B[38;5;241m.\u001B[39mrun_app()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\explainer\\smart_explainer.py:196\u001B[0m, in \u001B[0;36mSmartExplainer.__init__\u001B[1;34m(self, model, backend, preprocessing, postprocessing, features_groups, features_dict, label_dict, title_story, palette_name, colors_dict, **kwargs)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    195\u001B[0m     backend_cls \u001B[38;5;241m=\u001B[39m get_backend_cls_from_name(backend)\n\u001B[1;32m--> 196\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend_cls(\n\u001B[0;32m    197\u001B[0m         model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, preprocessing\u001B[38;5;241m=\u001B[39mpreprocessing, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, BaseBackend):\n\u001B[0;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\backend\\shap_backend.py:18\u001B[0m, in \u001B[0;36mShapBackend.__init__\u001B[1;34m(self, model, preprocessing, explainer_args, explainer_compute_args)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args \u001B[38;5;241m=\u001B[39m explainer_args \u001B[38;5;28;01mif\u001B[39;00m explainer_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_compute_args \u001B[38;5;241m=\u001B[39m explainer_compute_args \u001B[38;5;28;01mif\u001B[39;00m explainer_compute_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(model\u001B[38;5;241m=\u001B[39mmodel, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shap\\explainers\\_explainer.py:173\u001B[0m, in \u001B[0;36mExplainer.__init__\u001B[1;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001B[0m\n\u001B[0;32m    169\u001B[0m             algorithm \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpermutation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;66;03m# if we get here then we don't know how to handle what was given to us\u001B[39;00m\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe passed model is not callable and cannot be analyzed directly with the given masker! Model: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(model))\n\u001B[0;32m    175\u001B[0m \u001B[38;5;66;03m# build the right subclass\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexact\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mTypeError\u001B[0m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: SGDClassifier(loss='modified_huber', max_iter=500, penalty='l1',\n              random_state=42)"
     ]
    }
   ],
   "source": [
    "xpl = SmartExplainer(\n",
    "    model=model_sgd_3,\n",
    "    #features_dict=house_dict,  # Optional parameter\n",
    "    #preprocessing=encoder, # Optional: compile step can use inverse_transform method\n",
    "    #postprocessing=postprocess, # Optional: see tutorial postprocessing\n",
    ")\n",
    "xpl.compile(\n",
    "    x=X_test3,\n",
    "    #y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\n",
    "    y_target=y_test3,  # Optional: allows to display True Values vs Predicted Values\n",
    "    #additional_data=X_additional, # Optional: additional dataset of features for Webapp\n",
    "    #additional_features_dict=features_dict_additional, # Optional: dict additional data\n",
    ")\n",
    "app = xpl.run_app()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The passed model is not callable and cannot be analyzed directly with the given masker! Model: AdaBoostClassifier(learning_rate=0.1, n_estimators=500, random_state=34)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m xpl \u001B[38;5;241m=\u001B[39m \u001B[43mSmartExplainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_adaboost_3\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#features_dict=house_dict,  # Optional parameter\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#preprocessing=encoder, # Optional: compile step can use inverse_transform method\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#postprocessing=postprocess, # Optional: see tutorial postprocessing\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m xpl\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[0;32m      8\u001B[0m     x\u001B[38;5;241m=\u001B[39mX_test3,\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m#y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m#additional_features_dict=features_dict_additional, # Optional: dict additional data\u001B[39;00m\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     14\u001B[0m app \u001B[38;5;241m=\u001B[39m xpl\u001B[38;5;241m.\u001B[39mrun_app()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\explainer\\smart_explainer.py:196\u001B[0m, in \u001B[0;36mSmartExplainer.__init__\u001B[1;34m(self, model, backend, preprocessing, postprocessing, features_groups, features_dict, label_dict, title_story, palette_name, colors_dict, **kwargs)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    195\u001B[0m     backend_cls \u001B[38;5;241m=\u001B[39m get_backend_cls_from_name(backend)\n\u001B[1;32m--> 196\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend_cls(\n\u001B[0;32m    197\u001B[0m         model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, preprocessing\u001B[38;5;241m=\u001B[39mpreprocessing, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, BaseBackend):\n\u001B[0;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\backend\\shap_backend.py:18\u001B[0m, in \u001B[0;36mShapBackend.__init__\u001B[1;34m(self, model, preprocessing, explainer_args, explainer_compute_args)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args \u001B[38;5;241m=\u001B[39m explainer_args \u001B[38;5;28;01mif\u001B[39;00m explainer_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_compute_args \u001B[38;5;241m=\u001B[39m explainer_compute_args \u001B[38;5;28;01mif\u001B[39;00m explainer_compute_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(model\u001B[38;5;241m=\u001B[39mmodel, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shap\\explainers\\_explainer.py:173\u001B[0m, in \u001B[0;36mExplainer.__init__\u001B[1;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001B[0m\n\u001B[0;32m    169\u001B[0m             algorithm \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpermutation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;66;03m# if we get here then we don't know how to handle what was given to us\u001B[39;00m\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe passed model is not callable and cannot be analyzed directly with the given masker! Model: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(model))\n\u001B[0;32m    175\u001B[0m \u001B[38;5;66;03m# build the right subclass\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexact\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mTypeError\u001B[0m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: AdaBoostClassifier(learning_rate=0.1, n_estimators=500, random_state=34)"
     ]
    }
   ],
   "source": [
    "xpl = SmartExplainer(\n",
    "    model=model_adaboost_3,\n",
    "    #features_dict=house_dict,  # Optional parameter\n",
    "    #preprocessing=encoder, # Optional: compile step can use inverse_transform method\n",
    "    #postprocessing=postprocess, # Optional: see tutorial postprocessing\n",
    ")\n",
    "xpl.compile(\n",
    "    x=X_test3,\n",
    "    #y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\n",
    "    y_target=y_test3,  # Optional: allows to display True Values vs Predicted Values\n",
    "    #additional_data=X_additional, # Optional: additional dataset of features for Webapp\n",
    "    #additional_features_dict=features_dict_additional, # Optional: dict additional data\n",
    ")\n",
    "app = xpl.run_app()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPClassifier(random_state=34)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m xpl \u001B[38;5;241m=\u001B[39m \u001B[43mSmartExplainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_mlp_3\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#features_dict=house_dict,  # Optional parameter\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#preprocessing=encoder, # Optional: compile step can use inverse_transform method\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#postprocessing=postprocess, # Optional: see tutorial postprocessing\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m xpl\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[0;32m      8\u001B[0m     x\u001B[38;5;241m=\u001B[39mX_test3,\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m#y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m#additional_features_dict=features_dict_additional, # Optional: dict additional data\u001B[39;00m\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     14\u001B[0m app \u001B[38;5;241m=\u001B[39m xpl\u001B[38;5;241m.\u001B[39mrun_app()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\explainer\\smart_explainer.py:196\u001B[0m, in \u001B[0;36mSmartExplainer.__init__\u001B[1;34m(self, model, backend, preprocessing, postprocessing, features_groups, features_dict, label_dict, title_story, palette_name, colors_dict, **kwargs)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    195\u001B[0m     backend_cls \u001B[38;5;241m=\u001B[39m get_backend_cls_from_name(backend)\n\u001B[1;32m--> 196\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend_cls(\n\u001B[0;32m    197\u001B[0m         model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, preprocessing\u001B[38;5;241m=\u001B[39mpreprocessing, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, BaseBackend):\n\u001B[0;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\backend\\shap_backend.py:18\u001B[0m, in \u001B[0;36mShapBackend.__init__\u001B[1;34m(self, model, preprocessing, explainer_args, explainer_compute_args)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args \u001B[38;5;241m=\u001B[39m explainer_args \u001B[38;5;28;01mif\u001B[39;00m explainer_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_compute_args \u001B[38;5;241m=\u001B[39m explainer_compute_args \u001B[38;5;28;01mif\u001B[39;00m explainer_compute_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(model\u001B[38;5;241m=\u001B[39mmodel, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shap\\explainers\\_explainer.py:173\u001B[0m, in \u001B[0;36mExplainer.__init__\u001B[1;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001B[0m\n\u001B[0;32m    169\u001B[0m             algorithm \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpermutation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;66;03m# if we get here then we don't know how to handle what was given to us\u001B[39;00m\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe passed model is not callable and cannot be analyzed directly with the given masker! Model: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(model))\n\u001B[0;32m    175\u001B[0m \u001B[38;5;66;03m# build the right subclass\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexact\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mTypeError\u001B[0m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: MLPClassifier(random_state=34)"
     ]
    }
   ],
   "source": [
    "xpl = SmartExplainer(\n",
    "    model=model_mlp_3,\n",
    "    #features_dict=house_dict,  # Optional parameter\n",
    "    #preprocessing=encoder, # Optional: compile step can use inverse_transform method\n",
    "    #postprocessing=postprocess, # Optional: see tutorial postprocessing\n",
    ")\n",
    "xpl.compile(\n",
    "    x=X_test3,\n",
    "    #y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\n",
    "    y_target=y_test3,  # Optional: allows to display True Values vs Predicted Values\n",
    "    #additional_data=X_additional, # Optional: additional dataset of features for Webapp\n",
    "    #additional_features_dict=features_dict_additional, # Optional: dict additional data\n",
    ")\n",
    "app = xpl.run_app()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The passed model is not callable and cannot be analyzed directly with the given masker! Model: SVC(C=1, gamma=0.001, max_iter=10000, random_state=34)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m xpl \u001B[38;5;241m=\u001B[39m \u001B[43mSmartExplainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_svc_3\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#features_dict=house_dict,  # Optional parameter\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#preprocessing=encoder, # Optional: compile step can use inverse_transform method\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#postprocessing=postprocess, # Optional: see tutorial postprocessing\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m xpl\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[0;32m      8\u001B[0m     x\u001B[38;5;241m=\u001B[39mX_test3,\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m#y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m#additional_features_dict=features_dict_additional, # Optional: dict additional data\u001B[39;00m\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     14\u001B[0m app \u001B[38;5;241m=\u001B[39m xpl\u001B[38;5;241m.\u001B[39mrun_app()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\explainer\\smart_explainer.py:196\u001B[0m, in \u001B[0;36mSmartExplainer.__init__\u001B[1;34m(self, model, backend, preprocessing, postprocessing, features_groups, features_dict, label_dict, title_story, palette_name, colors_dict, **kwargs)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    195\u001B[0m     backend_cls \u001B[38;5;241m=\u001B[39m get_backend_cls_from_name(backend)\n\u001B[1;32m--> 196\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend_cls(\n\u001B[0;32m    197\u001B[0m         model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, preprocessing\u001B[38;5;241m=\u001B[39mpreprocessing, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, BaseBackend):\n\u001B[0;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\backend\\shap_backend.py:18\u001B[0m, in \u001B[0;36mShapBackend.__init__\u001B[1;34m(self, model, preprocessing, explainer_args, explainer_compute_args)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args \u001B[38;5;241m=\u001B[39m explainer_args \u001B[38;5;28;01mif\u001B[39;00m explainer_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_compute_args \u001B[38;5;241m=\u001B[39m explainer_compute_args \u001B[38;5;28;01mif\u001B[39;00m explainer_compute_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(model\u001B[38;5;241m=\u001B[39mmodel, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shap\\explainers\\_explainer.py:173\u001B[0m, in \u001B[0;36mExplainer.__init__\u001B[1;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001B[0m\n\u001B[0;32m    169\u001B[0m             algorithm \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpermutation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;66;03m# if we get here then we don't know how to handle what was given to us\u001B[39;00m\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe passed model is not callable and cannot be analyzed directly with the given masker! Model: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(model))\n\u001B[0;32m    175\u001B[0m \u001B[38;5;66;03m# build the right subclass\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexact\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mTypeError\u001B[0m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: SVC(C=1, gamma=0.001, max_iter=10000, random_state=34)"
     ]
    }
   ],
   "source": [
    "xpl = SmartExplainer(\n",
    "    model=model_svc_3,\n",
    "    #features_dict=house_dict,  # Optional parameter\n",
    "    #preprocessing=encoder, # Optional: compile step can use inverse_transform method\n",
    "    #postprocessing=postprocess, # Optional: see tutorial postprocessing\n",
    ")\n",
    "xpl.compile(\n",
    "    x=X_test3,\n",
    "    #y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\n",
    "    y_target=y_test3,  # Optional: allows to display True Values vs Predicted Values\n",
    "    #additional_data=X_additional, # Optional: additional dataset of features for Webapp\n",
    "    #additional_features_dict=features_dict_additional, # Optional: dict additional data\n",
    ")\n",
    "app = xpl.run_app()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The passed model is not callable and cannot be analyzed directly with the given masker! Model: KNeighborsClassifier(n_neighbors=9, p=1, weights='distance')",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m xpl \u001B[38;5;241m=\u001B[39m \u001B[43mSmartExplainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_knn_3\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#features_dict=house_dict,  # Optional parameter\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#preprocessing=encoder, # Optional: compile step can use inverse_transform method\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#postprocessing=postprocess, # Optional: see tutorial postprocessing\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m xpl\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[0;32m      8\u001B[0m     x\u001B[38;5;241m=\u001B[39mX_test3,\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m#y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m#additional_features_dict=features_dict_additional, # Optional: dict additional data\u001B[39;00m\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     14\u001B[0m app \u001B[38;5;241m=\u001B[39m xpl\u001B[38;5;241m.\u001B[39mrun_app()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\explainer\\smart_explainer.py:196\u001B[0m, in \u001B[0;36mSmartExplainer.__init__\u001B[1;34m(self, model, backend, preprocessing, postprocessing, features_groups, features_dict, label_dict, title_story, palette_name, colors_dict, **kwargs)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    195\u001B[0m     backend_cls \u001B[38;5;241m=\u001B[39m get_backend_cls_from_name(backend)\n\u001B[1;32m--> 196\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend_cls(\n\u001B[0;32m    197\u001B[0m         model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, preprocessing\u001B[38;5;241m=\u001B[39mpreprocessing, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, BaseBackend):\n\u001B[0;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\backend\\shap_backend.py:18\u001B[0m, in \u001B[0;36mShapBackend.__init__\u001B[1;34m(self, model, preprocessing, explainer_args, explainer_compute_args)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args \u001B[38;5;241m=\u001B[39m explainer_args \u001B[38;5;28;01mif\u001B[39;00m explainer_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_compute_args \u001B[38;5;241m=\u001B[39m explainer_compute_args \u001B[38;5;28;01mif\u001B[39;00m explainer_compute_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(model\u001B[38;5;241m=\u001B[39mmodel, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shap\\explainers\\_explainer.py:173\u001B[0m, in \u001B[0;36mExplainer.__init__\u001B[1;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001B[0m\n\u001B[0;32m    169\u001B[0m             algorithm \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpermutation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;66;03m# if we get here then we don't know how to handle what was given to us\u001B[39;00m\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe passed model is not callable and cannot be analyzed directly with the given masker! Model: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(model))\n\u001B[0;32m    175\u001B[0m \u001B[38;5;66;03m# build the right subclass\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexact\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mTypeError\u001B[0m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: KNeighborsClassifier(n_neighbors=9, p=1, weights='distance')"
     ]
    }
   ],
   "source": [
    "xpl = SmartExplainer(\n",
    "    model=model_knn_3,\n",
    "    #features_dict=house_dict,  # Optional parameter\n",
    "    #preprocessing=encoder, # Optional: compile step can use inverse_transform method\n",
    "    #postprocessing=postprocess, # Optional: see tutorial postprocessing\n",
    ")\n",
    "xpl.compile(\n",
    "    x=X_test3,\n",
    "    #y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\n",
    "    y_target=y_test3,  # Optional: allows to display True Values vs Predicted Values\n",
    "    #additional_data=X_additional, # Optional: additional dataset of features for Webapp\n",
    "    #additional_features_dict=features_dict_additional, # Optional: dict additional data\n",
    ")\n",
    "app = xpl.run_app()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The passed model is not callable and cannot be analyzed directly with the given masker! Model: LogisticRegression(C=10, random_state=33)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m xpl \u001B[38;5;241m=\u001B[39m \u001B[43mSmartExplainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_lr_3\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#features_dict=house_dict,  # Optional parameter\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#preprocessing=encoder, # Optional: compile step can use inverse_transform method\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#postprocessing=postprocess, # Optional: see tutorial postprocessing\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m xpl\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[0;32m      8\u001B[0m     x\u001B[38;5;241m=\u001B[39mX_test3,\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m#y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m#additional_features_dict=features_dict_additional, # Optional: dict additional data\u001B[39;00m\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     14\u001B[0m app \u001B[38;5;241m=\u001B[39m xpl\u001B[38;5;241m.\u001B[39mrun_app()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\explainer\\smart_explainer.py:196\u001B[0m, in \u001B[0;36mSmartExplainer.__init__\u001B[1;34m(self, model, backend, preprocessing, postprocessing, features_groups, features_dict, label_dict, title_story, palette_name, colors_dict, **kwargs)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    195\u001B[0m     backend_cls \u001B[38;5;241m=\u001B[39m get_backend_cls_from_name(backend)\n\u001B[1;32m--> 196\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend_cls(\n\u001B[0;32m    197\u001B[0m         model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, preprocessing\u001B[38;5;241m=\u001B[39mpreprocessing, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(backend, BaseBackend):\n\u001B[0;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackend \u001B[38;5;241m=\u001B[39m backend\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shapash\\backend\\shap_backend.py:18\u001B[0m, in \u001B[0;36mShapBackend.__init__\u001B[1;34m(self, model, preprocessing, explainer_args, explainer_compute_args)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args \u001B[38;5;241m=\u001B[39m explainer_args \u001B[38;5;28;01mif\u001B[39;00m explainer_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_compute_args \u001B[38;5;241m=\u001B[39m explainer_compute_args \u001B[38;5;28;01mif\u001B[39;00m explainer_compute_args \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(model\u001B[38;5;241m=\u001B[39mmodel, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer_args)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\shap\\explainers\\_explainer.py:173\u001B[0m, in \u001B[0;36mExplainer.__init__\u001B[1;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001B[0m\n\u001B[0;32m    169\u001B[0m             algorithm \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpermutation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;66;03m# if we get here then we don't know how to handle what was given to us\u001B[39;00m\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 173\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe passed model is not callable and cannot be analyzed directly with the given masker! Model: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(model))\n\u001B[0;32m    175\u001B[0m \u001B[38;5;66;03m# build the right subclass\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m algorithm \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexact\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mTypeError\u001B[0m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: LogisticRegression(C=10, random_state=33)"
     ]
    }
   ],
   "source": [
    "xpl = SmartExplainer(\n",
    "    model=model_lr_3,\n",
    "    #features_dict=house_dict,  # Optional parameter\n",
    "    #preprocessing=encoder, # Optional: compile step can use inverse_transform method\n",
    "    #postprocessing=postprocess, # Optional: see tutorial postprocessing\n",
    ")\n",
    "xpl.compile(\n",
    "    x=X_test3,\n",
    "    #y_pred=pd.DataFrame(y_pred_tree_3, index=X_test3.index), # Optional: for your own prediction (by default: model.predict)\n",
    "    y_target=y_test3,  # Optional: allows to display True Values vs Predicted Values\n",
    "    #additional_data=X_additional, # Optional: additional dataset of features for Webapp\n",
    "    #additional_features_dict=features_dict_additional, # Optional: dict additional data\n",
    ")\n",
    "app = xpl.run_app()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
