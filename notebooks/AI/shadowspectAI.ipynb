{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Import Libraries and modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../python')\n",
    "import splitDataset\n",
    "import splitDataset2\n",
    "np.set_printoptions(suppress=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data1= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil1.csv', sep=\";\")\n",
    "data2= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil2.csv', sep=\";\")\n",
    "data3= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil3.csv', sep=\";\")\n",
    "\n",
    "# want to eliminate the rows where feature bestSubmit is 100.0\n",
    "\n",
    "data1 = data1[data1.bestSubmit != 100.0]\n",
    "data2 = data2[data2.bestSubmit != 100.0]\n",
    "data3 = data3[data3.bestSubmit != 100.0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    3460\n",
      "0.0    1450\n",
      "Name: completed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts = data1['completed'].value_counts()\n",
    "\n",
    "print(counts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Split data into training and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows X_train1 dataset:  3295\n",
      "Number of rows X_test1 dataset:  1615\n",
      "Number of rows X_train2 dataset:  2551\n",
      "Number of rows X_test2 dataset:  1286\n",
      "Number of rows X_train3 dataset:  2079\n",
      "Number of rows X_test3 dataset:  971\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = splitDataset.splitDataset(data1)\n",
    "X_train2, X_test2, y_train2, y_test2 = splitDataset.splitDataset(data2)\n",
    "X_train3, X_test3, y_train3, y_test3 = splitDataset.splitDataset(data3)\n",
    "\n",
    "groups1 = splitDataset2.splitDataset2(X_train1)\n",
    "groups2 = splitDataset2.splitDataset2(X_train2)\n",
    "groups3 = splitDataset2.splitDataset2(X_train3)\n",
    "\n",
    "X_train1 = X_train1.drop(['user'], axis=1)\n",
    "X_test1 = X_test1.drop(['user'], axis=1)\n",
    "\n",
    "X_train2 = X_train2.drop(['user'], axis=1)\n",
    "X_test2 = X_test2.drop(['user'], axis=1)\n",
    "\n",
    "X_train3 = X_train3.drop(['user'], axis=1)\n",
    "X_test3 = X_test3.drop(['user'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# show the number of rows of X_train1\n",
    "print(\"Number of rows X_train1 dataset: \", X_train1.shape[0])\n",
    "print(\"Number of rows X_test1 dataset: \", X_test1.shape[0])\n",
    "\n",
    "print(\"Number of rows X_train2 dataset: \", X_train2.shape[0])\n",
    "print(\"Number of rows X_test2 dataset: \", X_test2.shape[0])\n",
    "\n",
    "print(\"Number of rows X_train3 dataset: \", X_train3.shape[0])\n",
    "print(\"Number of rows X_test3 dataset: \", X_test3.shape[0])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Declare data preprocessing steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Declare hyperparameters to tune"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "param_grid_rfc = {'n_estimators': [150],\n",
    "                    'max_features': [5, 7, 9],\n",
    "                    'max_depth'   : [None, 3, 10, 20],\n",
    "                    'criterion'   : ['gini', 'entropy','log_loss'],\n",
    "                    'max_leaf_nodes' : [3, 10, 20],\n",
    "                    }\n",
    "\n",
    "\n",
    "param_grid_rfc2 = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "# i want to make a param grid for the SGD Classifier\n",
    "param_grid_sgd = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "param_grid_sgd2 = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'l1_ratio': [0.15, 0.3, 0.5],\n",
    "    'fit_intercept': [True, False],\n",
    "    'max_iter': [1000, 2000],\n",
    "    'tol': [0.0001, 0.001],\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "    'eta0': [0.01, 0.1, 1.0],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "param_grid_mlp2 = {\n",
    "    'hidden_layer_sizes': [(100,), (50, 50), (50, 100, 50)],\n",
    "    'activation': ['relu', 'logistic', 'tanh'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'max_iter': [200, 500, 1000],\n",
    "    'tol': [0.0001, 0.001],\n",
    "    'early_stopping': [True, False],\n",
    "    'validation_fraction': [0.1, 0.2, 0.3],\n",
    "    'beta_1': [0.9, 0.95, 0.99],\n",
    "    'beta_2': [0.999, 0.9, 0.99]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "param_grid_adaboost = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "param_grid_adaboost2 = {\n",
    "    'base_estimator': [DecisionTreeClassifier(max_depth=1), LogisticRegression()],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.1, 0.5, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "param_grid_svc2 = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [2, 3, 4],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'coef0': [0.0, 0.5, 1.0],\n",
    "    'shrinking': [True, False],\n",
    "    'probability': [True, False],\n",
    "    'tol': [0.001, 0.0001],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_tree = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 3, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 3, 5]\n",
    "}\n",
    "\n",
    "param_grid_tree2 = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "param_grid_knn2 = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "    'leaf_size': [30, 40, 50],\n",
    "    'p': [1, 2],\n",
    "    'metric': ['euclidean', 'manhattan'],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Tune model using cross-validation pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1 Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 Random Forest classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1944 candidates, totalling 19440 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 34\u001B[0m\n\u001B[0;32m     12\u001B[0m grid_rfc_2 \u001B[38;5;241m=\u001B[39m GridSearchCV(\n\u001B[0;32m     13\u001B[0m     estimator  \u001B[38;5;241m=\u001B[39m RandomForestClassifier(random_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m123\u001B[39m),\n\u001B[0;32m     14\u001B[0m     param_grid \u001B[38;5;241m=\u001B[39m param_grid_rfc2,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     20\u001B[0m     return_train_score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     21\u001B[0m )\n\u001B[0;32m     23\u001B[0m grid_rfc_3 \u001B[38;5;241m=\u001B[39m GridSearchCV(\n\u001B[0;32m     24\u001B[0m     estimator  \u001B[38;5;241m=\u001B[39m RandomForestClassifier(random_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m123\u001B[39m),\n\u001B[0;32m     25\u001B[0m     param_grid \u001B[38;5;241m=\u001B[39m param_grid_rfc2,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     31\u001B[0m     return_train_score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     32\u001B[0m )\n\u001B[1;32m---> 34\u001B[0m grid_rfc_1 \u001B[38;5;241m=\u001B[39m \u001B[43mgrid_rfc_1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mX_train1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my_train1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mgroups1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m model_rfc_1 \u001B[38;5;241m=\u001B[39m grid_rfc_1\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[0;32m     36\u001B[0m grid_rfc_2 \u001B[38;5;241m=\u001B[39m grid_rfc_2\u001B[38;5;241m.\u001B[39mfit(X \u001B[38;5;241m=\u001B[39m X_train2, y \u001B[38;5;241m=\u001B[39m y_train2, groups \u001B[38;5;241m=\u001B[39m groups2)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[0;32m    868\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m    869\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m    870\u001B[0m     )\n\u001B[0;32m    872\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m--> 874\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m    877\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m    878\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1386\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1387\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1388\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    813\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    814\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    816\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    817\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    818\u001B[0m         )\n\u001B[0;32m    819\u001B[0m     )\n\u001B[1;32m--> 821\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    822\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    823\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    825\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    827\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    828\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    829\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    830\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    831\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    832\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    833\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    834\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    835\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    836\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    838\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    839\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    840\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    841\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    842\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    843\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     58\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     59\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     60\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     62\u001B[0m )\n\u001B[1;32m---> 63\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\joblib\\parallel.py:1098\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1095\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1098\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m elapsed_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_time\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\joblib\\parallel.py:975\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    974\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupports_timeout\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m--> 975\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(\u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    976\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    977\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget())\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001B[0m, in \u001B[0;36mLokyBackend.wrap_future_result\u001B[1;34m(future, timeout)\u001B[0m\n\u001B[0;32m    564\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001B[39;00m\n\u001B[0;32m    565\u001B[0m \u001B[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001B[39;00m\n\u001B[0;32m    566\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 567\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CfTimeoutError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\concurrent\\futures\\_base.py:441\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    438\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m    439\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[1;32m--> 441\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_condition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001B[0;32m    444\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\xAI\\lib\\threading.py:312\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[0;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 312\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    313\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "grid_rfc_1 = GridSearchCV(\n",
    "    estimator  = RandomForestClassifier(random_state = 123),\n",
    "    param_grid = param_grid_rfc2,\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_rfc_2 = GridSearchCV(\n",
    "    estimator  = RandomForestClassifier(random_state = 123),\n",
    "    param_grid = param_grid_rfc2,\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_rfc_3 = GridSearchCV(\n",
    "    estimator  = RandomForestClassifier(random_state = 123),\n",
    "    param_grid = param_grid_rfc2,\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_rfc_1 = grid_rfc_1.fit(X = X_train1, y = y_train1, groups = groups1)\n",
    "model_rfc_1 = grid_rfc_1.best_estimator_\n",
    "grid_rfc_2 = grid_rfc_2.fit(X = X_train2, y = y_train2, groups = groups2)\n",
    "model_rfc_2 = grid_rfc_2.best_estimator_\n",
    "grid_rfc_3 = grid_rfc_3.fit(X = X_train3, y = y_train3, groups = groups3)\n",
    "model_rfc_3 = grid_rfc_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo random forest para el percentil 1 es:  RandomForestClassifier(max_features=9, max_leaf_nodes=20, n_estimators=150,\n",
      "                       random_state=123)\n",
      "    param_criterion param_max_depth param_max_features param_max_leaf_nodes  \\\n",
      "8              gini            None                  9                   20   \n",
      "35             gini              20                  9                   20   \n",
      "26             gini              10                  9                   20   \n",
      "107        log_loss              20                  9                   20   \n",
      "80         log_loss            None                  9                   20   \n",
      "..              ...             ...                ...                  ...   \n",
      "72         log_loss            None                  5                    3   \n",
      "63          entropy              20                  5                    3   \n",
      "45          entropy               3                  5                    3   \n",
      "36          entropy            None                  5                    3   \n",
      "54          entropy              10                  5                    3   \n",
      "\n",
      "    param_n_estimators  mean_test_score  std_test_score  mean_train_score  \\\n",
      "8                  150         0.697526        0.071134          0.760057   \n",
      "35                 150         0.697526        0.071134          0.760057   \n",
      "26                 150         0.696766        0.071786          0.759815   \n",
      "107                150         0.694024        0.063340          0.748734   \n",
      "80                 150         0.694024        0.063340          0.748734   \n",
      "..                 ...              ...             ...               ...   \n",
      "72                 150         0.546510        0.049146          0.573331   \n",
      "63                 150         0.546510        0.049146          0.573331   \n",
      "45                 150         0.546510        0.049146          0.573331   \n",
      "36                 150         0.546510        0.049146          0.573331   \n",
      "54                 150         0.546510        0.049146          0.573331   \n",
      "\n",
      "     std_train_score  \n",
      "8           0.012721  \n",
      "35          0.012721  \n",
      "26          0.012305  \n",
      "107         0.014631  \n",
      "80          0.014631  \n",
      "..               ...  \n",
      "72          0.018095  \n",
      "63          0.018095  \n",
      "45          0.018095  \n",
      "36          0.018095  \n",
      "54          0.018095  \n",
      "\n",
      "[108 rows x 9 columns]\n",
      "El mejor modelo random forest para el percentil 2 es:  RandomForestClassifier(max_depth=10, max_features=9, max_leaf_nodes=20,\n",
      "                       n_estimators=150, random_state=123)\n",
      "   param_criterion param_max_depth param_max_features param_max_leaf_nodes  \\\n",
      "26            gini              10                  9                   20   \n",
      "8             gini            None                  9                   20   \n",
      "35            gini              20                  9                   20   \n",
      "23            gini              10                  7                   20   \n",
      "5             gini            None                  7                   20   \n",
      "..             ...             ...                ...                  ...   \n",
      "72        log_loss            None                  5                    3   \n",
      "63         entropy              20                  5                    3   \n",
      "45         entropy               3                  5                    3   \n",
      "36         entropy            None                  5                    3   \n",
      "54         entropy              10                  5                    3   \n",
      "\n",
      "   param_n_estimators  mean_test_score  std_test_score  mean_train_score  \\\n",
      "26                150         0.735396        0.069679          0.803322   \n",
      "8                 150         0.735068        0.070814          0.803593   \n",
      "35                150         0.735068        0.070814          0.803593   \n",
      "23                150         0.733775        0.076750          0.800852   \n",
      "5                 150         0.732762        0.077527          0.800959   \n",
      "..                ...              ...             ...               ...   \n",
      "72                150         0.620188        0.068974          0.663809   \n",
      "63                150         0.620188        0.068974          0.663809   \n",
      "45                150         0.620188        0.068974          0.663809   \n",
      "36                150         0.620188        0.068974          0.663809   \n",
      "54                150         0.620188        0.068974          0.663809   \n",
      "\n",
      "    std_train_score  \n",
      "26         0.008097  \n",
      "8          0.008271  \n",
      "35         0.008271  \n",
      "23         0.008263  \n",
      "5          0.008629  \n",
      "..              ...  \n",
      "72         0.016013  \n",
      "63         0.016013  \n",
      "45         0.016013  \n",
      "36         0.016013  \n",
      "54         0.016013  \n",
      "\n",
      "[108 rows x 9 columns]\n",
      "El mejor modelo random forest para el percentil 3 es:  RandomForestClassifier(max_depth=10, max_features=9, max_leaf_nodes=20,\n",
      "                       n_estimators=150, random_state=123)\n",
      "   param_criterion param_max_depth param_max_features param_max_leaf_nodes  \\\n",
      "26            gini              10                  9                   20   \n",
      "8             gini            None                  9                   20   \n",
      "35            gini              20                  9                   20   \n",
      "23            gini              10                  7                   20   \n",
      "68         entropy              20                  7                   20   \n",
      "..             ...             ...                ...                  ...   \n",
      "96        log_loss              10                  9                    3   \n",
      "30            gini              20                  7                    3   \n",
      "21            gini              10                  7                    3   \n",
      "12            gini               3                  7                    3   \n",
      "3             gini            None                  7                    3   \n",
      "\n",
      "   param_n_estimators  mean_test_score  std_test_score  mean_train_score  \\\n",
      "26                150         0.774804        0.066784          0.832886   \n",
      "8                 150         0.773861        0.067127          0.832623   \n",
      "35                150         0.773861        0.067127          0.832623   \n",
      "23                150         0.771034        0.068029          0.830394   \n",
      "68                150         0.770519        0.071487          0.828631   \n",
      "..                ...              ...             ...               ...   \n",
      "96                150         0.698684        0.066639          0.760917   \n",
      "30                150         0.696842        0.078635          0.759284   \n",
      "21                150         0.696842        0.078635          0.759284   \n",
      "12                150         0.696842        0.078635          0.759284   \n",
      "3                 150         0.696842        0.078635          0.759284   \n",
      "\n",
      "    std_train_score  \n",
      "26         0.004504  \n",
      "8          0.005232  \n",
      "35         0.005232  \n",
      "23         0.004156  \n",
      "68         0.004796  \n",
      "..              ...  \n",
      "96         0.008205  \n",
      "30         0.008323  \n",
      "21         0.008323  \n",
      "12         0.008323  \n",
      "3          0.008323  \n",
      "\n",
      "[108 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "resultados_rfc_1 = pd.DataFrame(grid_rfc_1.cv_results_)\n",
    "resultados_rfc_1 = resultados_rfc_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_rfc_1.to_csv(\"../../Outputs/grids/grid_rfc_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo random forest para el percentil 1 es: ', grid_rfc_1.best_estimator_)\n",
    "\n",
    "resultados_rfc_2 = pd.DataFrame(grid_rfc_2.cv_results_)\n",
    "resultados_rfc_2 = resultados_rfc_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_rfc_2.to_csv(\"../../Outputs/grids/grid_rfc_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo random forest para el percentil 2 es: ', grid_rfc_2.best_estimator_)\n",
    "\n",
    "resultados_rfc_3 = pd.DataFrame(grid_rfc_3.cv_results_)\n",
    "resultados_rfc_3 = resultados_rfc_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_rfc_3.to_csv(\"../../Outputs/grids/grid_rfc_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo random forest para el percentil 3 es: ', grid_rfc_3.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 Sthocastic Gradient Descent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grid_sgd_1 = GridSearchCV(\n",
    "    estimator  = SGDClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_sgd2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_sgd_2 = GridSearchCV(\n",
    "    estimator  = SGDClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_sgd2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_sgd_3 = GridSearchCV(\n",
    "    estimator  = SGDClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_sgd2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_sgd_1 = grid_sgd_1.fit(X = X_train1, y = y_train1, groups = groups1)\n",
    "model_sgd_1 = grid_sgd_1.best_estimator_\n",
    "grid_sgd_2 = grid_sgd_2.fit(X = X_train2, y = y_train2, groups = groups2)\n",
    "model_sgd_2 = grid_sgd_2.best_estimator_\n",
    "grid_sgd_3 = grid_sgd_3.fit(X = X_train3, y = y_train3, groups = groups3)\n",
    "model_sgd_3 = grid_sgd_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo SGD para el percentil 1 es:  SGDClassifier(loss='log', max_iter=500, penalty='l1', random_state=42)\n",
      "El mejor modelo SGD para el percentil 2 es:  SGDClassifier(alpha=0.001, loss='modified_huber', max_iter=100, penalty='l1',\n",
      "              random_state=42)\n",
      "El mejor modelo SGD para el percentil 3 es:  SGDClassifier(alpha=0.001, loss='log', max_iter=100, penalty='l1',\n",
      "              random_state=42)\n"
     ]
    }
   ],
   "source": [
    "resultados_sgd_1 = pd.DataFrame(grid_sgd_1.cv_results_)\n",
    "resultados_sgd_1 = resultados_sgd_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_sgd_1.to_csv(\"../../Outputs/grids/grid_sgd_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SGD para el percentil 1 es: ', grid_sgd_1.best_estimator_)\n",
    "\n",
    "resultados_sgd_2 = pd.DataFrame(grid_sgd_2.cv_results_)\n",
    "resultados_sgd_2 = resultados_sgd_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_sgd_2.to_csv(\"../../Outputs/grids/grid_sgd_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SGD para el percentil 2 es: ', grid_sgd_2.best_estimator_)\n",
    "\n",
    "resultados_sgd_3 = pd.DataFrame(grid_sgd_3.cv_results_)\n",
    "resultados_sgd_3 = resultados_sgd_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_sgd_3.to_csv(\"../../Outputs/grids/grid_sgd_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SGD para el percentil 3 es: ', grid_sgd_3.best_estimator_)\n",
    "\n",
    "# I want to know the balanced accuracy reached by the best model of every perecentile"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4 MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "grid_mlp_1 = GridSearchCV(\n",
    "    estimator  = MLPClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_mlp2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_mlp_2 = GridSearchCV(\n",
    "    estimator  = MLPClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_mlp2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_mlp_3 = GridSearchCV(\n",
    "    estimator  = MLPClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_mlp2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_mlp_1 = grid_mlp_1.fit(X = X_train1, y = y_train1, groups = groups1)\n",
    "model_mlp_1 = grid_mlp_1.best_estimator_\n",
    "grid_mlp_2 = grid_mlp_2.fit(X = X_train2, y = y_train2, groups = groups2)\n",
    "model_mlp_2 = grid_mlp_2.best_estimator_\n",
    "grid_mlp_3 = grid_mlp_3.fit(X = X_train3, y = y_train3, groups = groups3)\n",
    "model_mlp_3 = grid_mlp_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo MLP para el percentil 1 es:  MLPClassifier(alpha=0.05, hidden_layer_sizes=(50, 100, 50), random_state=34)\n",
      "El mejor modelo MLP para el percentil 2 es:  MLPClassifier(random_state=34)\n",
      "El mejor modelo MLP para el percentil 3 es:  MLPClassifier(random_state=34)\n",
      "Los mejores parámetros para el modelo MLP son:  {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "resultados_mlp_1 = pd.DataFrame(grid_mlp_1.cv_results_)\n",
    "resultados_mlp_1 = resultados_mlp_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_mlp_1.to_csv(\"../../Outputs/grids/grid_mlp_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo MLP para el percentil 1 es: ', grid_mlp_1.best_estimator_)\n",
    "\n",
    "resultados_mlp_2 = pd.DataFrame(grid_mlp_2.cv_results_)\n",
    "resultados_mlp_2 = resultados_mlp_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_mlp_2.to_csv(\"../../Outputs/grids/grid_mlp_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo MLP para el percentil 2 es: ', grid_mlp_2.best_estimator_)\n",
    "\n",
    "resultados_mlp_3 = pd.DataFrame(grid_mlp_3.cv_results_)\n",
    "resultados_mlp_3 = resultados_mlp_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_mlp_3.to_csv(\"../../Outputs/grids/grid_mlp_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo MLP para el percentil 3 es: ', grid_mlp_3.best_estimator_)\n",
    "print('Los mejores parámetros para el modelo MLP son: ', grid_mlp_3.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.5 AdaBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "grid_adaboost_1 = GridSearchCV(\n",
    "    estimator  = AdaBoostClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_adaboost2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_adaboost_2 = GridSearchCV(\n",
    "    estimator  = AdaBoostClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_adaboost2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_adaboost_3 = GridSearchCV(\n",
    "    estimator  = AdaBoostClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_adaboost2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_adaboost_1 = grid_adaboost_1.fit(X = X_train1, y = y_train1, groups = groups1)\n",
    "model_adaboost_1 = grid_adaboost_1.best_estimator_\n",
    "grid_adaboost_2 = grid_adaboost_2.fit(X = X_train2, y = y_train2, groups = groups2)\n",
    "model_adaboost_2 = grid_adaboost_2.best_estimator_\n",
    "grid_adaboost_3 = grid_adaboost_3.fit(X = X_train3, y = y_train3, groups = groups3)\n",
    "model_adaboost_3 = grid_adaboost_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo AdaBoost para el percentil 1 es:  AdaBoostClassifier(algorithm='SAMME', n_estimators=500, random_state=34)\n",
      "El mejor modelo AdaBoost para el percentil 2 es:  AdaBoostClassifier(n_estimators=100, random_state=34)\n",
      "El mejor modelo AdaBoost para el percentil 3 es:  AdaBoostClassifier(random_state=34)\n",
      "Los mejores parámetros para el modelo AdaBoost son:  {'algorithm': 'SAMME.R', 'learning_rate': 1.0, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "resultados_adaboost_1 = pd.DataFrame(grid_adaboost_1.cv_results_)\n",
    "resultados_adaboost_1 = resultados_adaboost_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_adaboost_1.to_csv(\"../../Outputs/grids/grid_adaboost_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo AdaBoost para el percentil 1 es: ', grid_adaboost_1.best_estimator_)\n",
    "\n",
    "resultados_adaboost_2 = pd.DataFrame(grid_adaboost_2.cv_results_)\n",
    "resultados_adaboost_2 = resultados_adaboost_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_adaboost_2.to_csv(\"../../Outputs/grids/grid_adaboost_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo AdaBoost para el percentil 2 es: ', grid_adaboost_2.best_estimator_)\n",
    "\n",
    "resultados_adaboost_3 = pd.DataFrame(grid_adaboost_3.cv_results_)\n",
    "resultados_adaboost_3 = resultados_adaboost_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_adaboost_3.to_csv(\"../../Outputs/grids/grid_adaboost_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo AdaBoost para el percentil 3 es: ', grid_adaboost_3.best_estimator_)\n",
    "print('Los mejores parámetros para el modelo AdaBoost son: ', grid_adaboost_3.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.6 K-Nearest Neighbors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "grid_knn_1 = GridSearchCV(\n",
    "    estimator  = KNeighborsClassifier(),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_knn2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_knn_2 = GridSearchCV(\n",
    "    estimator  = KNeighborsClassifier(),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_knn2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_knn_3 = GridSearchCV(\n",
    "    estimator  = KNeighborsClassifier(),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_knn2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_knn_1 = grid_knn_1.fit(X = X_train1, y = y_train1, groups = groups1)\n",
    "model_knn_1 = grid_knn_1.best_estimator_\n",
    "grid_knn_2 = grid_knn_2.fit(X = X_train2, y = y_train2, groups = groups2)\n",
    "model_knn_2 = grid_knn_2.best_estimator_\n",
    "grid_knn_3 = grid_knn_3.fit(X = X_train3, y = y_train3, groups = groups3)\n",
    "model_knn_3 = grid_knn_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo KNN para el percentil 1 es:  KNeighborsClassifier(n_neighbors=3, p=1)\n",
      "El mejor modelo KNN para el percentil 2 es:  KNeighborsClassifier(n_neighbors=3, p=1, weights='distance')\n",
      "El mejor modelo KNN para el percentil 3 es:  KNeighborsClassifier(n_neighbors=9, p=1, weights='distance')\n",
      "Los mejores parametros para KNN para el percentil 3 es:  {'algorithm': 'auto', 'n_neighbors': 9, 'p': 1, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "resultados_knn_1 = pd.DataFrame(grid_knn_1.cv_results_)\n",
    "resultados_knn_1 = resultados_knn_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_knn_1.to_csv(\"../../Outputs/grids/grid_knn_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo KNN para el percentil 1 es: ', grid_knn_1.best_estimator_)\n",
    "\n",
    "resultados_knn_2 = pd.DataFrame(grid_knn_2.cv_results_)\n",
    "resultados_knn_2 = resultados_knn_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_knn_2.to_csv(\"../../Outputs/grids/grid_knn_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo KNN para el percentil 2 es: ', grid_knn_2.best_estimator_)\n",
    "\n",
    "resultados_knn_3 = pd.DataFrame(grid_knn_3.cv_results_)\n",
    "resultados_knn_3 = resultados_knn_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_knn_3.to_csv(\"../../Outputs/grids/grid_knn_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo KNN para el percentil 3 es: ', grid_knn_3.best_estimator_)\n",
    "print('Los mejores parametros para KNN para el percentil 3 es: ', grid_knn_3.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.7 Tree decision classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "grid_tree_1 = GridSearchCV(\n",
    "    estimator  = DecisionTreeClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_tree2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_tree_2 = GridSearchCV(\n",
    "    estimator  = DecisionTreeClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_tree2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_tree_3 = GridSearchCV(\n",
    "    estimator  = DecisionTreeClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_tree2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_tree_1 = grid_tree_1.fit(X = X_train1, y = y_train1, groups=groups1)\n",
    "model_tree_1 = grid_tree_1.best_estimator_\n",
    "grid_tree_2 = grid_tree_2.fit(X = X_train2, y = y_train2, groups=groups2)\n",
    "model_tree_2 = grid_tree_2.best_estimator_\n",
    "grid_tree_3 = grid_tree_3.fit(X = X_train3, y = y_train3, groups=groups3)\n",
    "model_tree_3 = grid_tree_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo Árbol de Decisión para el percentil 1 es:  DecisionTreeClassifier(max_depth=20, min_samples_leaf=3, min_samples_split=10,\n",
      "                       random_state=34)\n",
      "El mejor modelo Árbol de Decisión para el percentil 2 es:  DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_split=5,\n",
      "                       random_state=34)\n",
      "El mejor modelo Árbol de Decisión para el percentil 3 es:  DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_split=10,\n",
      "                       random_state=34)\n"
     ]
    }
   ],
   "source": [
    "resultados_tree_1 = pd.DataFrame(grid_tree_1.cv_results_)\n",
    "resultados_tree_1 = resultados_tree_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_tree_1.to_csv(\"../../Outputs/grids/grid_tree_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo Árbol de Decisión para el percentil 1 es: ', grid_tree_1.best_estimator_)\n",
    "\n",
    "resultados_tree_2 = pd.DataFrame(grid_tree_2.cv_results_)\n",
    "resultados_tree_2 = resultados_tree_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_tree_2.to_csv(\"../../Outputs/grids/grid_tree_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo Árbol de Decisión para el percentil 2 es: ', grid_tree_2.best_estimator_)\n",
    "\n",
    "resultados_tree_3 = pd.DataFrame(grid_tree_3.cv_results_)\n",
    "resultados_tree_3 = resultados_tree_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_tree_3.to_csv(\"../../Outputs/grids/grid_tree_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo Árbol de Decisión para el percentil 3 es: ', grid_tree_3.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.8 Support vector classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    }
   ],
   "source": [
    "grid_svc_1 = GridSearchCV(\n",
    "    estimator  = SVC(random_state = 34,max_iter=10000),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_svc2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_svc_2 = GridSearchCV(\n",
    "    estimator  = SVC(random_state = 34,max_iter=10000),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_svc2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_svc_3 = GridSearchCV(\n",
    "    estimator  = SVC(random_state = 34,max_iter=10000),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_svc2,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = GroupKFold(n_splits = 10),\n",
    "    refit      = True,\n",
    "    verbose    = 1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_svc_1 = grid_svc_1.fit(X = X_train1, y = y_train1, groups=groups1)\n",
    "model_svc_1 = grid_svc_1.best_estimator_\n",
    "grid_svc_2 = grid_svc_2.fit(X = X_train2, y = y_train2, groups=groups2)\n",
    "model_svc_2 = grid_svc_2.best_estimator_\n",
    "grid_svc_3 = grid_svc_3.fit(X = X_train3, y = y_train3, groups=groups3)\n",
    "model_svc_3 = grid_svc_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo SVC para el percentil 1 es:  SVC(C=100, gamma=0.001, max_iter=10000, random_state=34)\n",
      "El mejor modelo SVC para el percentil 2 es:  SVC(C=100, gamma=0.001, max_iter=10000, random_state=34)\n",
      "El mejor modelo SVC para el percentil 3 es:  SVC(C=10, gamma=0.001, max_iter=10000, random_state=34)\n"
     ]
    }
   ],
   "source": [
    "resultados_svc_1 = pd.DataFrame(grid_svc_1.cv_results_)\n",
    "resultados_svc_1 = resultados_svc_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_svc_1.to_csv(\"../../Outputs/grids/grid_svc_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SVC para el percentil 1 es: ', grid_svc_1.best_estimator_)\n",
    "\n",
    "resultados_svc_2 = pd.DataFrame(grid_svc_2.cv_results_)\n",
    "resultados_svc_2 = resultados_svc_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_svc_2.to_csv(\"../../Outputs/grids/grid_svc_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SVC para el percentil 2 es: ', grid_svc_2.best_estimator_)\n",
    "\n",
    "resultados_svc_3 = pd.DataFrame(grid_svc_3.cv_results_)\n",
    "resultados_svc_3 = resultados_svc_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_svc_3.to_csv(\"../../Outputs/grids/grid_svc_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SVC para el percentil 3 es: ', grid_svc_3.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparación de modelos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Comparación de modelos ####\n",
      "#################################\n",
      "RandomForestClassifier\n",
      "Balanced Accuracy percentil 1:  0.6975260971028372\n",
      "Balanced Accuracy percentil 2:  0.7353960569198085\n",
      "Balanced Accuracy percentil 3:  0.7922757417690903\n",
      "#################################\n",
      "Stochastic Gradient Descent\n",
      "Balanced Accuracy percentil 1:  0.691967105316122\n",
      "Balanced Accuracy percentil 2:  0.6997300747490411\n",
      "Balanced Accuracy percentil 3:  0.692401610854762\n",
      "#################################\n",
      "Support Vector Classifier\n",
      "Balanced Accuracy percentil 1:  0.6908438422016241\n",
      "Balanced Accuracy percentil 2:  0.7185847187485483\n",
      "Balanced Accuracy percentil 3:  0.7338226664867661\n",
      "#################################\n",
      "Decision Tree Classifier\n",
      "Balanced Accuracy percentil 1:  0.7307185146396312\n",
      "Balanced Accuracy percentil 2:  0.7606199213180149\n",
      "Balanced Accuracy percentil 3:  0.7527115538451107\n",
      "#################################\n",
      "KNeighborsClassifier\n",
      "Balanced Accuracy percentil 1:  0.6744955663961238\n",
      "Balanced Accuracy percentil 2:  0.7000001357356859\n",
      "Balanced Accuracy percentil 3:  0.7237111749175359\n",
      "#################################\n",
      "Multilayer Perceptron\n",
      "Balanced Accuracy percentil 1:  0.7298689599647246\n",
      "Balanced Accuracy percentil 2:  0.7605817474623372\n",
      "Balanced Accuracy percentil 3:  0.7884817227257436\n",
      "#################################\n",
      "AdaBoostClassifier\n",
      "Balanced Accuracy percentil 1:  0.7262131787386474\n",
      "Balanced Accuracy percentil 2:  0.7620729446604557\n",
      "Balanced Accuracy percentil 3:  0.7789303578997605\n"
     ]
    }
   ],
   "source": [
    "file = open(\"resultados-entrenamiento.txt\", \"w\")  # Abre el archivo en modo de escritura\n",
    "\n",
    "print(\"#### Comparación de modelos ####\", file=file)\n",
    "print(\"#################################\", file=file)\n",
    "print(\"RandomForestClassifier\", file=file)\n",
    "print(\"Balanced Accuracy percentil 1: \", str(grid_rfc_1.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 2: \", str(grid_rfc_2.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 3: \", str(grid_rfc_3.best_score_), file=file)\n",
    "print(\"#################################\", file=file)\n",
    "print(\"Stochastic Gradient Descent\", file=file)\n",
    "print(\"Balanced Accuracy percentil 1: \", str(grid_sgd_1.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 2: \", str(grid_sgd_2.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 3: \", str(grid_sgd_3.best_score_), file=file)\n",
    "print(\"#################################\", file=file)\n",
    "print(\"Support Vector Classifier\", file=file)\n",
    "print(\"Balanced Accuracy percentil 1: \", str(grid_svc_1.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 2: \", str(grid_svc_2.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 3: \", str(grid_svc_3.best_score_), file=file)\n",
    "print(\"#################################\", file=file)\n",
    "print(\"Decision Tree Classifier\", file=file)\n",
    "print(\"Balanced Accuracy percentil 1: \", str(grid_tree_1.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 2: \", str(grid_tree_2.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 3: \", str(grid_tree_3.best_score_), file=file)\n",
    "print(\"#################################\", file=file)\n",
    "print(\"KNeighborsClassifier\", file=file)\n",
    "print(\"Balanced Accuracy percentil 1: \", str(grid_knn_1.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 2: \", str(grid_knn_2.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 3: \", str(grid_knn_3.best_score_), file=file)\n",
    "print(\"#################################\", file=file)\n",
    "print(\"Multilayer Perceptron\", file=file)\n",
    "print(\"Balanced Accuracy percentil 1: \", str(grid_mlp_1.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 2: \", str(grid_mlp_2.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 3: \", str(grid_mlp_3.best_score_), file=file)\n",
    "print(\"#################################\", file=file)\n",
    "print(\"AdaBoostClassifier\", file=file)\n",
    "print(\"Balanced Accuracy percentil 1: \", str(grid_adaboost_1.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 2: \", str(grid_adaboost_2.best_score_), file=file)\n",
    "print(\"Balanced Accuracy percentil 3: \", str(grid_adaboost_3.best_score_), file=file)\n",
    "\n",
    "file.close()  # Cierra el archivo cuando hayas terminado de escribir en él"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "['./models2/adaboost_3.pkl']"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_rfc_1, './models/rfc_1.pkl')\n",
    "joblib.dump(model_rfc_2, './models/rfc_2.pkl')\n",
    "joblib.dump(model_rfc_3, './models/rfc_3.pkl')\n",
    "\n",
    "joblib.dump(model_tree_1, './models/tree_1.pkl')\n",
    "joblib.dump(model_tree_2, './models/tree_2.pkl')\n",
    "joblib.dump(model_tree_3, './models/tree_3.pkl')\n",
    "\n",
    "joblib.dump(model_svc_1, './models/svc_1.pkl')\n",
    "joblib.dump(model_svc_2, './models/svc_2.pkl')\n",
    "joblib.dump(model_svc_3, './models/svc_3.pkl')\n",
    "\n",
    "joblib.dump(model_knn_1, './models/knn_1.pkl')\n",
    "joblib.dump(model_knn_2, './models/knn_2.pkl')\n",
    "joblib.dump(model_knn_3, './models/knn_3.pkl')\n",
    "\n",
    "joblib.dump(model_mlp_1,'./models/mlp_1.pkl')\n",
    "joblib.dump(model_mlp_2,'./models/mlp_2.pkl')\n",
    "joblib.dump(model_mlp_3,'./models/mlp_3.pkl')\n",
    "\n",
    "joblib.dump(model_sgd_1,'./models/sgd_1.pkl')\n",
    "joblib.dump(model_sgd_2,'./models/sgd_2.pkl')\n",
    "joblib.dump(model_sgd_3,'./models/sgd_3.pkl')\n",
    "\n",
    "joblib.dump(model_adaboost_1,'./models/adaboost_1.pkl')\n",
    "joblib.dump(model_adaboost_2,'./models/adaboost_2.pkl')\n",
    "joblib.dump(model_adaboost_3,'./models/adaboost_3.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
