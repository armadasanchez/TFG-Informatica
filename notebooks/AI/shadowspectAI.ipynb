{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Import Libraries and modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# how can i import a function to measure balanced accuracy?\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../python')\n",
    "import splitDataset\n",
    "np.set_printoptions(suppress=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data1= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil1.csv', sep=\";\")\n",
    "data2= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil2.csv', sep=\";\")\n",
    "data3= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil3.csv', sep=\";\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    3901\n",
      "0.0    1450\n",
      "Name: completed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts = data1['completed'].value_counts()\n",
    "\n",
    "print(counts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Split data into training and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows X_train1 dataset:  3577\n",
      "Number of rows X_test1 dataset:  1774\n",
      "Number of rows X_train2 dataset:  3577\n",
      "Number of rows X_test2 dataset:  1774\n",
      "Number of rows X_train3 dataset:  3577\n",
      "Number of rows X_test3 dataset:  1774\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = splitDataset.splitDataset(data1)\n",
    "X_train2, X_test2, y_train2, y_test2 = splitDataset.splitDataset(data2)\n",
    "X_train3, X_test3, y_train3, y_test3 = splitDataset.splitDataset(data3)\n",
    "\n",
    "# show the number of rows of X_train1\n",
    "print(\"Number of rows X_train1 dataset: \", X_train1.shape[0])\n",
    "print(\"Number of rows X_test1 dataset: \", X_test1.shape[0])\n",
    "\n",
    "print(\"Number of rows X_train2 dataset: \", X_train2.shape[0])\n",
    "print(\"Number of rows X_test2 dataset: \", X_test2.shape[0])\n",
    "\n",
    "print(\"Number of rows X_train3 dataset: \", X_train3.shape[0])\n",
    "print(\"Number of rows X_test3 dataset: \", X_test3.shape[0])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Declare data preprocessing steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Declare hyperparameters to tune"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "param_grid_lr = { 'penalty' : ['l1', 'l2','elasticnet',None],\n",
    "                    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "param_grid_rfc = {'n_estimators': [150],\n",
    "                    'max_features': [5, 7, 9],\n",
    "                    'max_depth'   : [None, 3, 10, 20],\n",
    "                    'criterion'   : ['gini', 'entropy']\n",
    "                    }\n",
    "# i want to make a param grid for the SGD Classifier\n",
    "param_grid_sgd = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "#param_grid_mlp = {'solver': ['lbfgs'], 'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15), 'random_state':[0,1,2,3,4,5,6,7,8,9]}\n",
    "\n",
    "param_grid_adaboost = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "param_grid_tree = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 3, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 3, 5]\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "    'p': [1, 2]\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Tune model using cross-validation pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1 Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.58499488        nan        nan        nan 0.62888097\n",
      "        nan        nan        nan 0.65550473        nan        nan\n",
      "        nan 0.66586263        nan        nan        nan 0.66736216\n",
      "        nan        nan        nan 0.67198073        nan        nan\n",
      "        nan 0.67123721        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan 0.59158854        nan        nan        nan 0.63640503\n",
      "        nan        nan        nan 0.66517581        nan        nan\n",
      "        nan 0.67514132        nan        nan        nan 0.67388857\n",
      "        nan        nan        nan 0.67855654        nan        nan\n",
      "        nan 0.67792759        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.66183078        nan        nan        nan 0.70292666\n",
      "        nan        nan        nan 0.72306683        nan        nan\n",
      "        nan 0.72291008        nan        nan        nan 0.72080725\n",
      "        nan        nan        nan 0.72396855        nan        nan\n",
      "        nan 0.72581968        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan 0.67155768        nan        nan        nan 0.71139607\n",
      "        nan        nan        nan 0.7291772         nan        nan\n",
      "        nan 0.73312013        nan        nan        nan 0.73273221\n",
      "        nan        nan        nan 0.73169992        nan        nan\n",
      "        nan 0.73012649        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "210 fits failed out of a total of 280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.73628184        nan        nan        nan 0.75691216\n",
      "        nan        nan        nan 0.7613649         nan        nan\n",
      "        nan 0.76693913        nan        nan        nan 0.77064979\n",
      "        nan        nan        nan 0.77231227        nan        nan\n",
      "        nan 0.7671278         nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan 0.74458626        nan        nan        nan 0.76723808\n",
      "        nan        nan        nan 0.77354432        nan        nan\n",
      "        nan 0.77733542        nan        nan        nan 0.78014087\n",
      "        nan        nan        nan 0.78000886        nan        nan\n",
      "        nan 0.77787343        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "grid_lr = GridSearchCV(\n",
    "    estimator  = LogisticRegression(random_state = 33),\n",
    "    param_grid = param_grid_lr,\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "\n",
    "grid_lr_1 = grid_lr.fit(X_train1, y_train1)\n",
    "model_lr_1 = grid_lr_1.best_estimator_\n",
    "grid_lr_2 = grid_lr.fit(X_train2, y_train2)\n",
    "model_lr_2 = grid_lr_2.best_estimator_\n",
    "grid_lr_3 = grid_lr.fit(X_train3, y_train3)\n",
    "model_lr_3 = grid_lr_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 Random Forest classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "   param_criterion param_max_depth param_max_features param_n_estimators  \\\n22         entropy              20                  7                150   \n13         entropy            None                  7                150   \n2             gini            None                  9                150   \n10            gini              20                  7                150   \n14         entropy            None                  9                150   \n23         entropy              20                  9                150   \n11            gini              20                  9                150   \n1             gini            None                  7                150   \n9             gini              20                  5                150   \n12         entropy            None                  5                150   \n0             gini            None                  5                150   \n20         entropy              10                  9                150   \n21         entropy              20                  5                150   \n8             gini              10                  9                150   \n19         entropy              10                  7                150   \n7             gini              10                  7                150   \n6             gini              10                  5                150   \n18         entropy              10                  5                150   \n5             gini               3                  9                150   \n17         entropy               3                  9                150   \n4             gini               3                  7                150   \n16         entropy               3                  7                150   \n3             gini               3                  5                150   \n15         entropy               3                  5                150   \n\n    mean_test_score  std_test_score  mean_train_score  std_train_score  \n22         0.839638        0.043649          0.999198         0.000783  \n13         0.838710        0.043349          1.000000         0.000000  \n2          0.838697        0.044272          1.000000         0.000000  \n10         0.837234        0.044087          0.998889         0.000864  \n14         0.837025        0.048512          1.000000         0.000000  \n23         0.835919        0.045020          0.999568         0.000556  \n11         0.835170        0.045349          0.999691         0.000414  \n1          0.834621        0.044715          1.000000         0.000000  \n9          0.833733        0.040167          0.996173         0.001763  \n12         0.831699        0.047309          1.000000         0.000000  \n0          0.828169        0.042414          1.000000         0.000000  \n20         0.827266        0.050552          0.894841         0.004666  \n21         0.826881        0.043689          0.997531         0.001461  \n8          0.821327        0.047720          0.899446         0.004582  \n19         0.820795        0.048424          0.887481         0.003781  \n7          0.820243        0.050251          0.892677         0.003639  \n6          0.808597        0.055860          0.876904         0.004380  \n18         0.804710        0.049901          0.871278         0.007027  \n5          0.716998        0.044273          0.727948         0.009948  \n17         0.710891        0.042195          0.720774         0.009428  \n4          0.699614        0.039320          0.706818         0.006657  \n16         0.692952        0.036221          0.698427         0.005260  \n3          0.687582        0.038738          0.695773         0.005926  \n15         0.679622        0.038355          0.689521         0.006384  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>param_criterion</th>\n      <th>param_max_depth</th>\n      <th>param_max_features</th>\n      <th>param_n_estimators</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>mean_train_score</th>\n      <th>std_train_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22</th>\n      <td>entropy</td>\n      <td>20</td>\n      <td>7</td>\n      <td>150</td>\n      <td>0.839638</td>\n      <td>0.043649</td>\n      <td>0.999198</td>\n      <td>0.000783</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>entropy</td>\n      <td>None</td>\n      <td>7</td>\n      <td>150</td>\n      <td>0.838710</td>\n      <td>0.043349</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>gini</td>\n      <td>None</td>\n      <td>9</td>\n      <td>150</td>\n      <td>0.838697</td>\n      <td>0.044272</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>gini</td>\n      <td>20</td>\n      <td>7</td>\n      <td>150</td>\n      <td>0.837234</td>\n      <td>0.044087</td>\n      <td>0.998889</td>\n      <td>0.000864</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>entropy</td>\n      <td>None</td>\n      <td>9</td>\n      <td>150</td>\n      <td>0.837025</td>\n      <td>0.048512</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>entropy</td>\n      <td>20</td>\n      <td>9</td>\n      <td>150</td>\n      <td>0.835919</td>\n      <td>0.045020</td>\n      <td>0.999568</td>\n      <td>0.000556</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>gini</td>\n      <td>20</td>\n      <td>9</td>\n      <td>150</td>\n      <td>0.835170</td>\n      <td>0.045349</td>\n      <td>0.999691</td>\n      <td>0.000414</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gini</td>\n      <td>None</td>\n      <td>7</td>\n      <td>150</td>\n      <td>0.834621</td>\n      <td>0.044715</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>gini</td>\n      <td>20</td>\n      <td>5</td>\n      <td>150</td>\n      <td>0.833733</td>\n      <td>0.040167</td>\n      <td>0.996173</td>\n      <td>0.001763</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>entropy</td>\n      <td>None</td>\n      <td>5</td>\n      <td>150</td>\n      <td>0.831699</td>\n      <td>0.047309</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>gini</td>\n      <td>None</td>\n      <td>5</td>\n      <td>150</td>\n      <td>0.828169</td>\n      <td>0.042414</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>entropy</td>\n      <td>10</td>\n      <td>9</td>\n      <td>150</td>\n      <td>0.827266</td>\n      <td>0.050552</td>\n      <td>0.894841</td>\n      <td>0.004666</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>entropy</td>\n      <td>20</td>\n      <td>5</td>\n      <td>150</td>\n      <td>0.826881</td>\n      <td>0.043689</td>\n      <td>0.997531</td>\n      <td>0.001461</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>gini</td>\n      <td>10</td>\n      <td>9</td>\n      <td>150</td>\n      <td>0.821327</td>\n      <td>0.047720</td>\n      <td>0.899446</td>\n      <td>0.004582</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>entropy</td>\n      <td>10</td>\n      <td>7</td>\n      <td>150</td>\n      <td>0.820795</td>\n      <td>0.048424</td>\n      <td>0.887481</td>\n      <td>0.003781</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>gini</td>\n      <td>10</td>\n      <td>7</td>\n      <td>150</td>\n      <td>0.820243</td>\n      <td>0.050251</td>\n      <td>0.892677</td>\n      <td>0.003639</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>gini</td>\n      <td>10</td>\n      <td>5</td>\n      <td>150</td>\n      <td>0.808597</td>\n      <td>0.055860</td>\n      <td>0.876904</td>\n      <td>0.004380</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>entropy</td>\n      <td>10</td>\n      <td>5</td>\n      <td>150</td>\n      <td>0.804710</td>\n      <td>0.049901</td>\n      <td>0.871278</td>\n      <td>0.007027</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>gini</td>\n      <td>3</td>\n      <td>9</td>\n      <td>150</td>\n      <td>0.716998</td>\n      <td>0.044273</td>\n      <td>0.727948</td>\n      <td>0.009948</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>entropy</td>\n      <td>3</td>\n      <td>9</td>\n      <td>150</td>\n      <td>0.710891</td>\n      <td>0.042195</td>\n      <td>0.720774</td>\n      <td>0.009428</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>gini</td>\n      <td>3</td>\n      <td>7</td>\n      <td>150</td>\n      <td>0.699614</td>\n      <td>0.039320</td>\n      <td>0.706818</td>\n      <td>0.006657</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>entropy</td>\n      <td>3</td>\n      <td>7</td>\n      <td>150</td>\n      <td>0.692952</td>\n      <td>0.036221</td>\n      <td>0.698427</td>\n      <td>0.005260</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gini</td>\n      <td>3</td>\n      <td>5</td>\n      <td>150</td>\n      <td>0.687582</td>\n      <td>0.038738</td>\n      <td>0.695773</td>\n      <td>0.005926</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>entropy</td>\n      <td>3</td>\n      <td>5</td>\n      <td>150</td>\n      <td>0.679622</td>\n      <td>0.038355</td>\n      <td>0.689521</td>\n      <td>0.006384</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rfc = GridSearchCV(\n",
    "    estimator  = RandomForestClassifier(random_state = 123),\n",
    "    param_grid = param_grid_rfc,\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_rfc_1 = grid_rfc.fit(X = X_train1, y = y_train1)\n",
    "model_rfc_1 = grid_rfc_1.best_estimator_\n",
    "grid_rfc_2 = grid_rfc.fit(X = X_train2, y = y_train2)\n",
    "model_rfc_2 = grid_rfc_2.best_estimator_\n",
    "grid_rfc_3 = grid_rfc.fit(X = X_train3, y = y_train3)\n",
    "model_rfc_3 = grid_rfc_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo es:  RandomForestClassifier(criterion='entropy', max_depth=20, max_features=7,\n",
      "                       n_estimators=150, random_state=123)\n"
     ]
    }
   ],
   "source": [
    "resultados_rfc_1 = pd.DataFrame(grid_rfc_1.cv_results_)\n",
    "resultados_rfc_1 = resultados_rfc_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_rfc_1.to_csv(\"../../Outputs/grids/grid_rfc_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo random forest para el percentil 1 es: ', grid_rfc_1.best_estimator_)\n",
    "\n",
    "resultados_rfc_2 = pd.DataFrame(grid_rfc_2.cv_results_)\n",
    "resultados_rfc_2 = resultados_rfc_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_rfc_2.to_csv(\"../../Outputs/grids/grid_rfc_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo random forest para el percentil 2 es: ', grid_rfc_2.best_estimator_)\n",
    "\n",
    "resultados_rfc_3 = pd.DataFrame(grid_rfc_3.cv_results_)\n",
    "resultados_rfc_3 = resultados_rfc_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_rfc_3.to_csv(\"../../Outputs/grids/grid_rfc_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo random forest para el percentil 3 es: ', grid_rfc_3.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 Sthocastic Gradient Descent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grid_sgd = GridSearchCV(\n",
    "    estimator  = SGDClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_sgd,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "grid_sgd_1 = grid_sgd.fit(X = X_train1, y = y_train1)\n",
    "model_sgd_1 = grid_sgd_1.best_estimator_\n",
    "grid_sgd_2 = grid_sgd.fit(X = X_train2, y = y_train2)\n",
    "model_sgd_2 = grid_sgd_2.best_estimator_\n",
    "grid_sgd_3 = grid_sgd.fit(X = X_train3, y = y_train3)\n",
    "model_sgd_3 = grid_sgd_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo SGD para el percentil 1 es:  SGDClassifier(max_iter=100, penalty='l1', random_state=42)\n",
      "El mejor modelo SGD para el percentil 2 es:  SGDClassifier(max_iter=100, penalty='l1', random_state=42)\n",
      "El mejor modelo SGD para el percentil 3 es:  SGDClassifier(max_iter=100, penalty='l1', random_state=42)\n"
     ]
    }
   ],
   "source": [
    "resultados_sgd_1 = pd.DataFrame(grid_sgd_1.cv_results_)\n",
    "resultados_sgd_1 = resultados_sgd_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_sgd_1.to_csv(\"../../Outputs/grids/grid_sgd_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SGD para el percentil 1 es: ', grid_sgd_1.best_estimator_)\n",
    "\n",
    "resultados_sgd_2 = pd.DataFrame(grid_sgd_2.cv_results_)\n",
    "resultados_sgd_2 = resultados_sgd_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_sgd_2.to_csv(\"../../Outputs/grids/grid_sgd_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SGD para el percentil 2 es: ', grid_sgd_2.best_estimator_)\n",
    "\n",
    "resultados_sgd_3 = pd.DataFrame(grid_sgd_3.cv_results_)\n",
    "resultados_sgd_3 = resultados_sgd_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_sgd_3.to_csv(\"../../Outputs/grids/grid_sgd_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SGD para el percentil 3 es: ', grid_sgd_3.best_estimator_)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4 MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grid_mlp = GridSearchCV(\n",
    "    estimator  = MLPClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_mlp,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_mlp_1 = grid_mlp.fit(X = X_train1, y = y_train1)\n",
    "model_mlp_1 = grid_mlp_1.best_estimator_\n",
    "grid_mlp_2 = grid_mlp.fit(X = X_train2, y = y_train2)\n",
    "model_mlp_2 = grid_mlp_2.best_estimator_\n",
    "grid_mlp_3 = grid_mlp.fit(X = X_train3, y = y_train3)\n",
    "model_mlp_3 = grid_mlp_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo MLP para el percentil 1 es:  MLPClassifier(random_state=34)\n",
      "El mejor modelo MLP para el percentil 2 es:  MLPClassifier(random_state=34)\n",
      "El mejor modelo MLP para el percentil 3 es:  MLPClassifier(random_state=34)\n",
      "Los mejores parámetros para el modelo MLP son:  {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "resultados_mlp_1 = pd.DataFrame(grid_mlp_1.cv_results_)\n",
    "resultados_mlp_1 = resultados_mlp_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_mlp_1.to_csv(\"../../Outputs/grids/grid_mlp_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo MLP para el percentil 1 es: ', grid_mlp_1.best_estimator_)\n",
    "\n",
    "resultados_mlp_2 = pd.DataFrame(grid_mlp_2.cv_results_)\n",
    "resultados_mlp_2 = resultados_mlp_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_mlp_2.to_csv(\"../../Outputs/grids/grid_mlp_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo MLP para el percentil 2 es: ', grid_mlp_2.best_estimator_)\n",
    "\n",
    "resultados_mlp_3 = pd.DataFrame(grid_mlp_3.cv_results_)\n",
    "resultados_mlp_3 = resultados_mlp_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_mlp_3.to_csv(\"../../Outputs/grids/grid_mlp_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo MLP para el percentil 3 es: ', grid_mlp_3.best_estimator_)\n",
    "print('Los mejores parámetros para el modelo MLP son: ', grid_mlp_3.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.5 AdaBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "grid_adaboost = GridSearchCV(\n",
    "    estimator  = AdaBoostClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_adaboost,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "\n",
    "grid_adaboost_1 = grid_adaboost.fit(X = X_train1, y = y_train1)\n",
    "model_adaboost_1 = grid_adaboost_1.best_estimator_\n",
    "grid_adaboost_2 = grid_adaboost.fit(X = X_train2, y = y_train2)\n",
    "model_adaboost_2 = grid_adaboost_2.best_estimator_\n",
    "grid_adaboost_3 = grid_adaboost.fit(X = X_train3, y = y_train3)\n",
    "model_adaboost_3 = grid_adaboost_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo AdaBoost para el percentil 1 es:  AdaBoostClassifier(algorithm='SAMME', n_estimators=200, random_state=34)\n",
      "El mejor modelo AdaBoost para el percentil 2 es:  AdaBoostClassifier(algorithm='SAMME', n_estimators=200, random_state=34)\n",
      "El mejor modelo AdaBoost para el percentil 3 es:  AdaBoostClassifier(algorithm='SAMME', n_estimators=200, random_state=34)\n",
      "Los mejores parámetros para el modelo AdaBoost son:  {'algorithm': 'SAMME', 'learning_rate': 1.0, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "resultados_adaboost_1 = pd.DataFrame(grid_adaboost_1.cv_results_)\n",
    "resultados_adaboost_1 = resultados_adaboost_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_adaboost_1.to_csv(\"../../Outputs/grids/grid_adaboost_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo AdaBoost para el percentil 1 es: ', grid_adaboost_1.best_estimator_)\n",
    "\n",
    "resultados_adaboost_2 = pd.DataFrame(grid_adaboost_2.cv_results_)\n",
    "resultados_adaboost_2 = resultados_adaboost_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_adaboost_2.to_csv(\"../../Outputs/grids/grid_adaboost_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo AdaBoost para el percentil 2 es: ', grid_adaboost_2.best_estimator_)\n",
    "\n",
    "resultados_adaboost_3 = pd.DataFrame(grid_adaboost_3.cv_results_)\n",
    "resultados_adaboost_3 = resultados_adaboost_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_adaboost_3.to_csv(\"../../Outputs/grids/grid_adaboost_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo AdaBoost para el percentil 3 es: ', grid_adaboost_3.best_estimator_)\n",
    "print('Los mejores parámetros para el modelo AdaBoost son: ', grid_adaboost_3.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.6 K-Nearest Neighbors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "grid_knn = GridSearchCV(\n",
    "    estimator  = KNeighborsClassifier(),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_knn,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_knn_1 = grid_knn.fit(X = X_train1, y = y_train1)\n",
    "model_knn_1 = grid_knn_1.best_estimator_\n",
    "grid_knn_2 = grid_knn.fit(X = X_train2, y = y_train2)\n",
    "model_knn_2 = grid_knn_2.best_estimator_\n",
    "grid_knn_3 = grid_knn.fit(X = X_train3, y = y_train3)\n",
    "model_knn_3 = grid_knn_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo KNN para el percentil 1 es:  KNeighborsClassifier(algorithm='ball_tree', n_neighbors=3, p=1)\n",
      "El mejor modelo KNN para el percentil 2 es:  KNeighborsClassifier(algorithm='ball_tree', n_neighbors=3, p=1)\n",
      "El mejor modelo KNN para el percentil 3 es:  KNeighborsClassifier(algorithm='ball_tree', n_neighbors=3, p=1)\n",
      "Los mejores parametros para KNN para el percentil 3 es:  {'algorithm': 'ball_tree', 'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "resultados_knn_1 = pd.DataFrame(grid_knn_1.cv_results_)\n",
    "resultados_knn_1 = resultados_knn_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_knn_1.to_csv(\"../../Outputs/grids/grid_knn_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo KNN para el percentil 1 es: ', grid_knn_1.best_estimator_)\n",
    "\n",
    "resultados_knn_2 = pd.DataFrame(grid_knn_2.cv_results_)\n",
    "resultados_knn_2 = resultados_knn_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_knn_2.to_csv(\"../../Outputs/grids/grid_knn_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo KNN para el percentil 2 es: ', grid_knn_2.best_estimator_)\n",
    "\n",
    "resultados_knn_3 = pd.DataFrame(grid_knn_3.cv_results_)\n",
    "resultados_knn_3 = resultados_knn_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_knn_3.to_csv(\"../../Outputs/grids/grid_knn_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo KNN para el percentil 3 es: ', grid_knn_3.best_estimator_)\n",
    "print('Los mejores parametros para KNN para el percentil 3 es: ', grid_knn_3.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.7 Tree decision classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "grid_tree = GridSearchCV(\n",
    "    estimator  = DecisionTreeClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_tree,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_tree_1 = grid_tree.fit(X = X_train1, y = y_train1)\n",
    "model_tree_1 = grid_tree_1.best_estimator_\n",
    "grid_tree_2 = grid_tree.fit(X = X_train2, y = y_train2)\n",
    "model_tree_2 = grid_tree_2.best_estimator_\n",
    "grid_tree_3 = grid_tree.fit(X = X_train3, y = y_train3)\n",
    "model_tree_3 = grid_tree_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo Árbol de Decisión para el percentil 1 es:  DecisionTreeClassifier(max_depth=20, min_samples_split=10, random_state=34)\n",
      "El mejor modelo Árbol de Decisión para el percentil 2 es:  DecisionTreeClassifier(max_depth=20, min_samples_split=10, random_state=34)\n",
      "El mejor modelo Árbol de Decisión para el percentil 3 es:  DecisionTreeClassifier(max_depth=20, min_samples_split=10, random_state=34)\n"
     ]
    }
   ],
   "source": [
    "resultados_tree_1 = pd.DataFrame(grid_tree_1.cv_results_)\n",
    "resultados_tree_1 = resultados_tree_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_tree_1.to_csv(\"../../Outputs/grids/grid_tree_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo Árbol de Decisión para el percentil 1 es: ', grid_tree_1.best_estimator_)\n",
    "\n",
    "resultados_tree_2 = pd.DataFrame(grid_tree_2.cv_results_)\n",
    "resultados_tree_2 = resultados_tree_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_tree_2.to_csv(\"../../Outputs/grids/grid_tree_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo Árbol de Decisión para el percentil 2 es: ', grid_tree_2.best_estimator_)\n",
    "\n",
    "resultados_tree_3 = pd.DataFrame(grid_tree_3.cv_results_)\n",
    "resultados_tree_3 = resultados_tree_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_tree_3.to_csv(\"../../Outputs/grids/grid_tree_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo Árbol de Decisión para el percentil 3 es: ', grid_tree_3.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.8 Support vector classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grid_svc = GridSearchCV(\n",
    "    estimator  = SVC(random_state = 34,max_iter=10000),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_svc,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_svc_1 = grid_svc.fit(X = X_train1, y = y_train1)\n",
    "model_svc_1 = grid_svc_1.best_estimator_\n",
    "grid_svc_2 = grid_svc.fit(X = X_train2, y = y_train2)\n",
    "model_svc_2 = grid_svc_2.best_estimator_\n",
    "grid_svc_3 = grid_svc.fit(X = X_train3, y = y_train3)\n",
    "model_svc_3 = grid_svc_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo SVC para el percentil 1 es:  SVC(C=100, gamma=0.001, max_iter=10000, random_state=34)\n",
      "El mejor modelo SVC para el percentil 2 es:  SVC(C=100, gamma=0.001, max_iter=10000, random_state=34)\n",
      "El mejor modelo SVC para el percentil 3 es:  SVC(C=100, gamma=0.001, max_iter=10000, random_state=34)\n"
     ]
    }
   ],
   "source": [
    "resultados_svc_1 = pd.DataFrame(grid_svc_1.cv_results_)\n",
    "resultados_svc_1 = resultados_svc_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_svc_1.to_csv(\"../../Outputs/grids/grid_svc_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SVC para el percentil 1 es: ', grid_svc_1.best_estimator_)\n",
    "\n",
    "resultados_svc_2 = pd.DataFrame(grid_svc_2.cv_results_)\n",
    "resultados_svc_2 = resultados_svc_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_svc_2.to_csv(\"../../Outputs/grids/grid_svc_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SVC para el percentil 2 es: ', grid_svc_2.best_estimator_)\n",
    "\n",
    "resultados_svc_3 = pd.DataFrame(grid_svc_3.cv_results_)\n",
    "resultados_svc_3 = resultados_svc_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_svc_3.to_csv(\"../../Outputs/grids/grid_svc_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SVC para el percentil 3 es: ', grid_svc_3.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Refit on the entire training set\n",
    "# No additional code needed if clf.refit == True (default is True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate model pipeline on test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "y_pred_lr_1 = model_lr_1.predict(X_test1)\n",
    "y_pred_binary_lr_1 = np.where(y_pred_lr_1 > 0.5, 1, 0)\n",
    "\n",
    "y_pred_lr_2 = model_lr_2.predict(X_test2)\n",
    "y_pred_binary_lr_2 = np.where(y_pred_lr_2 > 0.5, 1, 0)\n",
    "\n",
    "y_pred_lr_3 = model_lr_3.predict(X_test3)\n",
    "y_pred_binary_lr_3 = np.where(y_pred_lr_3 > 0.5, 1, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using lr is : 0.6837759952465835\n",
      "The balanced accuracy for percentile 2 using lr is : 0.7438680926916221\n",
      "The balanced accuracy for percentile 3 using lr is : 0.7950014854426619\n",
      "The f1 score for percentile 1 using lr is : 0.8531938737392605\n",
      "The f1 score for percentile 2 using lr is : 0.8742331288343558\n",
      "The f1 score for percentile 3 using lr is : 0.8948808128175069\n",
      "The cohen kappa score for percentile 1 using lr is : 0.4151422670449493\n",
      "The cohen kappa score for percentile 2 using lr is : 0.530202816173621\n",
      "The cohen kappa score for percentile 3 using lr is : 0.6247159059631044\n",
      "The confusion matrix for percentile 1 using lr is : [[ 239  311]\n",
      " [  82 1142]]\n",
      "The confusion matrix for percentile 2 using lr is : [[ 306  244]\n",
      " [  84 1140]]\n",
      "The confusion matrix for percentile 3 using lr is : [[ 360  190]\n",
      " [  79 1145]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using lr is : ' + str(balanced_accuracy_score(y_test1, y_pred_binary_lr_1)))\n",
    "print('The balanced accuracy for percentile 2 using lr is : ' + str(balanced_accuracy_score(y_test2, y_pred_binary_lr_2)))\n",
    "print('The balanced accuracy for percentile 3 using lr is : ' + str(balanced_accuracy_score(y_test3, y_pred_binary_lr_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using lr is : ' + str(f1_score(y_test1, y_pred_binary_lr_1)))\n",
    "print('The f1 score for percentile 2 using lr is : ' + str(f1_score(y_test2, y_pred_binary_lr_2)))\n",
    "print('The f1 score for percentile 3 using lr is : ' + str(f1_score(y_test3, y_pred_binary_lr_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using lr is : ' + str(cohen_kappa_score(y_test1, y_pred_binary_lr_1)))\n",
    "print('The cohen kappa score for percentile 2 using lr is : ' + str(cohen_kappa_score(y_test2, y_pred_binary_lr_2)))\n",
    "print('The cohen kappa score for percentile 3 using lr is : ' + str(cohen_kappa_score(y_test3, y_pred_binary_lr_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using lr is : ' + str(confusion_matrix(y_test1, y_pred_binary_lr_1)))\n",
    "print('The confusion matrix for percentile 2 using lr is : ' + str(confusion_matrix(y_test2, y_pred_binary_lr_2)))\n",
    "print('The confusion matrix for percentile 3 using lr is : ' + str(confusion_matrix(y_test3, y_pred_binary_lr_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "y_pred_rfc_1 = model_rfc_1.predict(X_test1)\n",
    "y_pred_rfc_2 = model_rfc_2.predict(X_test2)\n",
    "y_pred_rfc_3 = model_rfc_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using rfc is : 0.7585338680926916\n",
      "The balanced accuracy for percentile 2 using rfc is : 0.7835398098633393\n",
      "The balanced accuracy for percentile 3 using rfc is : 0.8165314913844326\n",
      "The f1 score for percentile 1 using rfc is : 0.8871151653363741\n",
      "The f1 score for percentile 2 using rfc is : 0.8937813827732716\n",
      "The f1 score for percentile 3 using rfc is : 0.910221531286436\n",
      "The cohen kappa score for percentile 1 using rfc is : 0.5692125802701098\n",
      "The cohen kappa score for percentile 2 using rfc is : 0.610146032415776\n",
      "The cohen kappa score for percentile 3 using rfc is : 0.6753206829616132\n",
      "The confusion matrix for percentile 1 using rfc is : [[ 310  240]\n",
      " [  57 1167]]\n",
      "The confusion matrix for percentile 2 using rfc is : [[ 342  208]\n",
      " [  67 1157]]\n",
      "The confusion matrix for percentile 3 using rfc is : [[ 372  178]\n",
      " [  53 1171]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using rfc is : ' + str(balanced_accuracy_score(y_test1, y_pred_rfc_1)))\n",
    "print('The balanced accuracy for percentile 2 using rfc is : ' + str(balanced_accuracy_score(y_test2, y_pred_rfc_2)))\n",
    "print('The balanced accuracy for percentile 3 using rfc is : ' + str(balanced_accuracy_score(y_test3, y_pred_rfc_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using rfc is : ' + str(f1_score(y_test1, y_pred_rfc_1)))\n",
    "print('The f1 score for percentile 2 using rfc is : ' + str(f1_score(y_test2, y_pred_rfc_2)))\n",
    "print('The f1 score for percentile 3 using rfc is : ' + str(f1_score(y_test3, y_pred_rfc_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using rfc is : ' + str(cohen_kappa_score(y_test1, y_pred_rfc_1)))\n",
    "print('The cohen kappa score for percentile 2 using rfc is : ' + str(cohen_kappa_score(y_test2, y_pred_rfc_2)))\n",
    "print('The cohen kappa score for percentile 3 using rfc is : ' + str(cohen_kappa_score(y_test3, y_pred_rfc_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using rfc is : ' + str(confusion_matrix(y_test1, y_pred_rfc_1)))\n",
    "print('The confusion matrix for percentile 2 using rfc is : ' + str(confusion_matrix(y_test2, y_pred_rfc_2)))\n",
    "print('The confusion matrix for percentile 3 using rfc is : ' + str(confusion_matrix(y_test3, y_pred_rfc_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "y_pred_sgd_1 = model_sgd_1.predict(X_test1)\n",
    "y_pred_sgd_2 = model_sgd_2.predict(X_test2)\n",
    "y_pred_sgd_3 = model_sgd_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using sgd is : 0.7437715389185977\n",
      "The balanced accuracy for percentile 2 using sgd is : 0.7964067142008319\n",
      "The balanced accuracy for percentile 3 using sgd is : 0.7771360665478313\n",
      "The f1 score for percentile 1 using sgd is : 0.8546395858223814\n",
      "The f1 score for percentile 2 using sgd is : 0.8754071661237786\n",
      "The f1 score for percentile 3 using sgd is : 0.8888888888888888\n",
      "The cohen kappa score for percentile 1 using sgd is : 0.5034196411179059\n",
      "The cohen kappa score for percentile 2 using sgd is : 0.5951970458043501\n",
      "The cohen kappa score for percentile 3 using sgd is : 0.5944448427207047\n",
      "The confusion matrix for percentile 1 using sgd is : [[ 336  214]\n",
      " [ 151 1073]]\n",
      "The confusion matrix for percentile 2 using sgd is : [[ 393  157]\n",
      " [ 149 1075]]\n",
      "The confusion matrix for percentile 3 using sgd is : [[ 339  211]\n",
      " [  76 1148]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using sgd is : ' + str(balanced_accuracy_score(y_test1, y_pred_sgd_1)))\n",
    "print('The balanced accuracy for percentile 2 using sgd is : ' + str(balanced_accuracy_score(y_test2, y_pred_sgd_2)))\n",
    "print('The balanced accuracy for percentile 3 using sgd is : ' + str(balanced_accuracy_score(y_test3, y_pred_sgd_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using sgd is : ' + str(f1_score(y_test1, y_pred_sgd_1)))\n",
    "print('The f1 score for percentile 2 using sgd is : ' + str(f1_score(y_test2, y_pred_sgd_2)))\n",
    "print('The f1 score for percentile 3 using sgd is : ' + str(f1_score(y_test3, y_pred_sgd_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using sgd is : ' + str(cohen_kappa_score(y_test1, y_pred_sgd_1)))\n",
    "print('The cohen kappa score for percentile 2 using sgd is : ' + str(cohen_kappa_score(y_test2, y_pred_sgd_2)))\n",
    "print('The cohen kappa score for percentile 3 using sgd is : ' + str(cohen_kappa_score(y_test3, y_pred_sgd_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using sgd is : ' + str(confusion_matrix(y_test1, y_pred_sgd_1)))\n",
    "print('The confusion matrix for percentile 2 using sgd is : ' + str(confusion_matrix(y_test2, y_pred_sgd_2)))\n",
    "print('The confusion matrix for percentile 3 using sgd is : ' + str(confusion_matrix(y_test3, y_pred_sgd_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "y_pred_mlp_1 = model_mlp_1.predict(X_test1)\n",
    "y_pred_mlp_2 = model_mlp_2.predict(X_test2)\n",
    "y_pred_mlp_3 = model_mlp_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using mlp is : 0.7630510992275699\n",
      "The balanced accuracy for percentile 2 using mlp is : 0.775634284016637\n",
      "The balanced accuracy for percentile 3 using mlp is : 0.8126708259061199\n",
      "The f1 score for percentile 1 using mlp is : 0.8808049535603716\n",
      "The f1 score for percentile 2 using mlp is : 0.8891463885670143\n",
      "The f1 score for percentile 3 using mlp is : 0.9073929961089494\n",
      "The cohen kappa score for percentile 1 using mlp is : 0.5645362849236811\n",
      "The cohen kappa score for percentile 2 using mlp is : 0.5931342229211918\n",
      "The cohen kappa score for percentile 3 using mlp is : 0.6660169660457596\n",
      "The confusion matrix for percentile 1 using mlp is : [[ 328  222]\n",
      " [  86 1138]]\n",
      "The confusion matrix for percentile 2 using mlp is : [[ 336  214]\n",
      " [  73 1151]]\n",
      "The confusion matrix for percentile 3 using mlp is : [[ 370  180]\n",
      " [  58 1166]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using mlp is : ' + str(balanced_accuracy_score(y_test1, y_pred_mlp_1)))\n",
    "print('The balanced accuracy for percentile 2 using mlp is : ' + str(balanced_accuracy_score(y_test2, y_pred_mlp_2)))\n",
    "print('The balanced accuracy for percentile 3 using mlp is : ' + str(balanced_accuracy_score(y_test3, y_pred_mlp_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using mlp is : ' + str(f1_score(y_test1, y_pred_mlp_1)))\n",
    "print('The f1 score for percentile 2 using mlp is : ' + str(f1_score(y_test2, y_pred_mlp_2)))\n",
    "print('The f1 score for percentile 3 using mlp is : ' + str(f1_score(y_test3, y_pred_mlp_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using mlp is : ' + str(cohen_kappa_score(y_test1, y_pred_mlp_1)))\n",
    "print('The cohen kappa score for percentile 2 using mlp is : ' + str(cohen_kappa_score(y_test2, y_pred_mlp_2)))\n",
    "print('The cohen kappa score for percentile 3 using mlp is : ' + str(cohen_kappa_score(y_test3, y_pred_mlp_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using mlp is : ' + str(confusion_matrix(y_test1, y_pred_mlp_1)))\n",
    "print('The confusion matrix for percentile 2 using mlp is : ' + str(confusion_matrix(y_test2, y_pred_mlp_2)))\n",
    "print('The confusion matrix for percentile 3 using mlp is : ' + str(confusion_matrix(y_test3, y_pred_mlp_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "y_pred_adaboost_1 = model_adaboost_1.predict(X_test1)\n",
    "y_pred_adaboost_2 = model_adaboost_2.predict(X_test2)\n",
    "y_pred_adaboost_3 = model_adaboost_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR ADABOOST\n",
      "\n",
      "-----Balanced accuracy-----\n",
      "percentile 1 : 0.7711244800950683\n",
      "percentile 2 : 0.8062789661319073\n",
      "percentile 3 : 0.8390062388591801\n",
      "\n",
      "-----F1 score-----\n",
      "percentile 1 : 0.8703629836457918\n",
      "percentile 2 : 0.8970414201183432\n",
      "percentile 3 : 0.9099639855942376\n",
      "\n",
      "-----Cohen kappa score-----\n",
      "percentile 1 : 0.5587517238951382\n",
      "percentile 2 : 0.6404506422770667\n",
      "percentile 3 : 0.6957758459054927\n",
      "\n",
      "-----Confusion matrix-----\n",
      "percentile 1 : [[ 358  192]\n",
      " [ 133 1091]]\n",
      "percentile 2 : [[ 376  174]\n",
      " [  87 1137]]\n",
      "percentile 3 : [[ 412  138]\n",
      " [  87 1137]]\n"
     ]
    }
   ],
   "source": [
    "print('RESULTS FOR ADABOOST')\n",
    "print()\n",
    "print('-----Balanced accuracy-----')\n",
    "print('percentile 1 : ' + str(balanced_accuracy_score(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(balanced_accuracy_score(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(balanced_accuracy_score(y_test3, y_pred_adaboost_3)))\n",
    "print()\n",
    "print('-----F1 score-----')\n",
    "print('percentile 1 : ' + str(f1_score(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(f1_score(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(f1_score(y_test3, y_pred_adaboost_3)))\n",
    "print()\n",
    "print('-----Cohen kappa score-----')\n",
    "print('percentile 1 : ' + str(cohen_kappa_score(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(cohen_kappa_score(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(cohen_kappa_score(y_test3, y_pred_adaboost_3)))\n",
    "print()\n",
    "print('-----Confusion matrix-----')\n",
    "print('percentile 1 : ' + str(confusion_matrix(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(confusion_matrix(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(confusion_matrix(y_test3, y_pred_adaboost_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\alvar\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "y_pred_knn_1 = model_knn_1.predict(X_test1)\n",
    "y_pred_knn_2 = model_knn_2.predict(X_test2)\n",
    "y_pred_knn_3 = model_knn_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using knn is : 0.6806001188354129\n",
      "The balanced accuracy for percentile 2 using knn is : 0.7470959595959596\n",
      "The balanced accuracy for percentile 3 using knn is : 0.7742245989304812\n",
      "The f1 score for percentile 1 using knn is : 0.849212303075769\n",
      "The f1 score for percentile 2 using knn is : 0.8751440645409143\n",
      "The f1 score for percentile 3 using knn is : 0.8888888888888888\n",
      "The cohen kappa score for percentile 1 using knn is : 0.40544641457712915\n",
      "The cohen kappa score for percentile 2 using knn is : 0.5357628851867657\n",
      "The cohen kappa score for percentile 3 using knn is : 0.591055786076533\n",
      "The confusion matrix for percentile 1 using knn is : [[ 240  310]\n",
      " [  92 1132]]\n",
      "The confusion matrix for percentile 2 using knn is : [[ 310  240]\n",
      " [  85 1139]]\n",
      "The confusion matrix for percentile 3 using knn is : [[ 334  216]\n",
      " [  72 1152]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using knn is : ' + str(balanced_accuracy_score(y_test1, y_pred_knn_1)))\n",
    "print('The balanced accuracy for percentile 2 using knn is : ' + str(balanced_accuracy_score(y_test2, y_pred_knn_2)))\n",
    "print('The balanced accuracy for percentile 3 using knn is : ' + str(balanced_accuracy_score(y_test3, y_pred_knn_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using knn is : ' + str(f1_score(y_test1, y_pred_knn_1)))\n",
    "print('The f1 score for percentile 2 using knn is : ' + str(f1_score(y_test2, y_pred_knn_2)))\n",
    "print('The f1 score for percentile 3 using knn is : ' + str(f1_score(y_test3, y_pred_knn_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using knn is : ' + str(cohen_kappa_score(y_test1, y_pred_knn_1)))\n",
    "print('The cohen kappa score for percentile 2 using knn is : ' + str(cohen_kappa_score(y_test2, y_pred_knn_2)))\n",
    "print('The cohen kappa score for percentile 3 using knn is : ' + str(cohen_kappa_score(y_test3, y_pred_knn_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using knn is : ' + str(confusion_matrix(y_test1, y_pred_knn_1)))\n",
    "print('The confusion matrix for percentile 2 using knn is : ' + str(confusion_matrix(y_test2, y_pred_knn_2)))\n",
    "print('The confusion matrix for percentile 3 using knn is : ' + str(confusion_matrix(y_test3, y_pred_knn_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "y_pred_tree_1 = model_tree_1.predict(X_test1)\n",
    "y_pred_tree_2 = model_tree_2.predict(X_test2)\n",
    "y_pred_tree_3 = model_tree_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using tree is : 0.7366711229946523\n",
      "The balanced accuracy for percentile 2 using tree is : 0.7810323826500297\n",
      "The balanced accuracy for percentile 3 using tree is : 0.8159863339275104\n",
      "The f1 score for percentile 1 using tree is : 0.856805664830842\n",
      "The f1 score for percentile 2 using tree is : 0.874749899959984\n",
      "The f1 score for percentile 3 using tree is : 0.8887083671811535\n",
      "The cohen kappa score for percentile 1 using tree is : 0.4967156231586758\n",
      "The cohen kappa score for percentile 2 using tree is : 0.5767903989707521\n",
      "The cohen kappa score for percentile 3 using tree is : 0.6364329929601694\n",
      "The confusion matrix for percentile 1 using tree is : [[ 321  229]\n",
      " [ 135 1089]]\n",
      "The confusion matrix for percentile 2 using tree is : [[ 368  182]\n",
      " [ 131 1093]]\n",
      "The confusion matrix for percentile 3 using tree is : [[ 406  144]\n",
      " [ 130 1094]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using tree is : ' + str(balanced_accuracy_score(y_test1, y_pred_tree_1)))\n",
    "print('The balanced accuracy for percentile 2 using tree is : ' + str(balanced_accuracy_score(y_test2, y_pred_tree_2)))\n",
    "print('The balanced accuracy for percentile 3 using tree is : ' + str(balanced_accuracy_score(y_test3, y_pred_tree_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using tree is : ' + str(f1_score(y_test1, y_pred_tree_1)))\n",
    "print('The f1 score for percentile 2 using tree is : ' + str(f1_score(y_test2, y_pred_tree_2)))\n",
    "print('The f1 score for percentile 3 using tree is : ' + str(f1_score(y_test3, y_pred_tree_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using tree is : ' + str(cohen_kappa_score(y_test1, y_pred_tree_1)))\n",
    "print('The cohen kappa score for percentile 2 using tree is : ' + str(cohen_kappa_score(y_test2, y_pred_tree_2)))\n",
    "print('The cohen kappa score for percentile 3 using tree is : ' + str(cohen_kappa_score(y_test3, y_pred_tree_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using tree is : ' + str(confusion_matrix(y_test1, y_pred_tree_1)))\n",
    "print('The confusion matrix for percentile 2 using tree is : ' + str(confusion_matrix(y_test2, y_pred_tree_2)))\n",
    "print('The confusion matrix for percentile 3 using tree is : ' + str(confusion_matrix(y_test3, y_pred_tree_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "y_pred_svc_1 = model_svc_1.predict(X_test1)\n",
    "y_pred_svc_2 = model_svc_2.predict(X_test2)\n",
    "y_pred_svc_3 = model_svc_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using svc is : 0.7055941770647653\n",
      "The balanced accuracy for percentile 2 using svc is : 0.7613770053475936\n",
      "The balanced accuracy for percentile 3 using svc is : 0.7937358882947119\n",
      "The f1 score for percentile 1 using svc is : 0.8609121748963439\n",
      "The f1 score for percentile 2 using svc is : 0.8765095442150369\n",
      "The f1 score for percentile 3 using svc is : 0.8910735351946522\n",
      "The cohen kappa score for percentile 1 using svc is : 0.4582107711280138\n",
      "The cohen kappa score for percentile 2 using svc is : 0.5558674263185578\n",
      "The cohen kappa score for percentile 3 using svc is : 0.616804822321171\n",
      "The confusion matrix for percentile 1 using svc is : [[ 263  287]\n",
      " [  82 1142]]\n",
      "The confusion matrix for percentile 2 using svc is : [[ 332  218]\n",
      " [  99 1125]]\n",
      "The confusion matrix for percentile 3 using svc is : [[ 364  186]\n",
      " [  91 1133]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using svc is : ' + str(balanced_accuracy_score(y_test1, y_pred_svc_1)))\n",
    "print('The balanced accuracy for percentile 2 using svc is : ' + str(balanced_accuracy_score(y_test2, y_pred_svc_2)))\n",
    "print('The balanced accuracy for percentile 3 using svc is : ' + str(balanced_accuracy_score(y_test3, y_pred_svc_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using svc is : ' + str(f1_score(y_test1, y_pred_svc_1)))\n",
    "print('The f1 score for percentile 2 using svc is : ' + str(f1_score(y_test2, y_pred_svc_2)))\n",
    "print('The f1 score for percentile 3 using svc is : ' + str(f1_score(y_test3, y_pred_svc_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using svc is : ' + str(cohen_kappa_score(y_test1, y_pred_svc_1)))\n",
    "print('The cohen kappa score for percentile 2 using svc is : ' + str(cohen_kappa_score(y_test2, y_pred_svc_2)))\n",
    "print('The cohen kappa score for percentile 3 using svc is : ' + str(cohen_kappa_score(y_test3, y_pred_svc_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using svc is : ' + str(confusion_matrix(y_test1, y_pred_svc_1)))\n",
    "print('The confusion matrix for percentile 2 using svc is : ' + str(confusion_matrix(y_test2, y_pred_svc_2)))\n",
    "print('The confusion matrix for percentile 3 using svc is : ' + str(confusion_matrix(y_test3, y_pred_svc_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importancia de los predictores\n",
    "\n",
    "##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importancia de los predictores en el modelo\n",
      "-------------------------------------------\n",
      "                   predictor  importancia\n",
      "7                   n_events     0.138790\n",
      "3        attempts_per_puzzle     0.132895\n",
      "11     n_manipulation_events     0.117894\n",
      "6          puzzle_difficulty     0.081645\n",
      "5                 puzzle_elo     0.077760\n",
      "4                   user_elo     0.072974\n",
      "13             n_rotate_view     0.046477\n",
      "1    percentage_intermediate     0.044532\n",
      "0        percentage_tutorial     0.042584\n",
      "2        percentage_advanced     0.038942\n",
      "9                 bestSubmit     0.031710\n",
      "10                  n_breaks     0.027051\n",
      "8           n_check_solution     0.019684\n",
      "37                   Sandbox     0.019209\n",
      "25               Bear Market     0.013042\n",
      "12                n_snapshot     0.012504\n",
      "14                1. One Box     0.007562\n",
      "40               Sugar Cones     0.005162\n",
      "33              Orange Dance     0.004545\n",
      "44                   Warm Up     0.003943\n",
      "26                  Bird Fez     0.003746\n",
      "38     Square Cross-Sections     0.003373\n",
      "35      Pyramids are Strange     0.003299\n",
      "15        2. Separated Boxes     0.003197\n",
      "34                  Pi Henge     0.003123\n",
      "21            7. Max 2 Boxes     0.002840\n",
      "32             Object Limits     0.002742\n",
      "27     Boxes Obscure Spheres     0.002601\n",
      "24         Angled Silhouette     0.002566\n",
      "19       5. Removing Objects     0.002417\n",
      "18       45-Degree Rotations     0.002407\n",
      "23  9. Scaling Round Objects     0.002379\n",
      "22        8. Combine 2 Ramps     0.002356\n",
      "20         6. Stretch a Ramp     0.002261\n",
      "31                  Not Bird     0.002219\n",
      "41            Tall and Small     0.002127\n",
      "16       3. Rotate a Pyramid     0.002113\n",
      "39           Stranger Shapes     0.002072\n",
      "45                       Zzz     0.002062\n",
      "36        Ramp Up and Can It     0.002037\n",
      "17      4. Match Silhouettes     0.002013\n",
      "30  More Than Meets Your Eye     0.001823\n",
      "28               Bull Market     0.001548\n",
      "29                 Few Clues     0.001439\n",
      "43               Unnecessary     0.001381\n",
      "42                 Tetromino     0.000954\n",
      "Importancia de los predictores en el modelo\n",
      "-------------------------------------------\n",
      "                   predictor  importancia\n",
      "11     n_manipulation_events     0.140791\n",
      "7                   n_events     0.132643\n",
      "3        attempts_per_puzzle     0.099346\n",
      "6          puzzle_difficulty     0.086739\n",
      "9                 bestSubmit     0.084479\n",
      "5                 puzzle_elo     0.068297\n",
      "4                   user_elo     0.056399\n",
      "13             n_rotate_view     0.048214\n",
      "8           n_check_solution     0.037978\n",
      "2        percentage_advanced     0.037623\n",
      "1    percentage_intermediate     0.033822\n",
      "0        percentage_tutorial     0.031011\n",
      "10                  n_breaks     0.027864\n",
      "37                   Sandbox     0.019881\n",
      "12                n_snapshot     0.017063\n",
      "14                1. One Box     0.011948\n",
      "25               Bear Market     0.010711\n",
      "40               Sugar Cones     0.004266\n",
      "33              Orange Dance     0.003160\n",
      "27     Boxes Obscure Spheres     0.002765\n",
      "21            7. Max 2 Boxes     0.002558\n",
      "38     Square Cross-Sections     0.002375\n",
      "26                  Bird Fez     0.002358\n",
      "24         Angled Silhouette     0.002297\n",
      "32             Object Limits     0.002266\n",
      "19       5. Removing Objects     0.002245\n",
      "44                   Warm Up     0.002189\n",
      "35      Pyramids are Strange     0.002171\n",
      "23  9. Scaling Round Objects     0.002113\n",
      "41            Tall and Small     0.002017\n",
      "31                  Not Bird     0.001833\n",
      "15        2. Separated Boxes     0.001775\n",
      "39           Stranger Shapes     0.001743\n",
      "16       3. Rotate a Pyramid     0.001707\n",
      "22        8. Combine 2 Ramps     0.001680\n",
      "34                  Pi Henge     0.001639\n",
      "20         6. Stretch a Ramp     0.001610\n",
      "18       45-Degree Rotations     0.001549\n",
      "45                       Zzz     0.001539\n",
      "36        Ramp Up and Can It     0.001408\n",
      "17      4. Match Silhouettes     0.001235\n",
      "30  More Than Meets Your Eye     0.001178\n",
      "29                 Few Clues     0.001086\n",
      "43               Unnecessary     0.001038\n",
      "28               Bull Market     0.001025\n",
      "42                 Tetromino     0.000366\n",
      "Importancia de los predictores en el modelo\n",
      "-------------------------------------------\n",
      "                   predictor  importancia\n",
      "9                 bestSubmit     0.202029\n",
      "7                   n_events     0.125374\n",
      "11     n_manipulation_events     0.118545\n",
      "3        attempts_per_puzzle     0.081191\n",
      "6          puzzle_difficulty     0.063914\n",
      "5                 puzzle_elo     0.062121\n",
      "8           n_check_solution     0.055224\n",
      "13             n_rotate_view     0.047909\n",
      "4                   user_elo     0.043453\n",
      "1    percentage_intermediate     0.027223\n",
      "10                  n_breaks     0.026206\n",
      "0        percentage_tutorial     0.026185\n",
      "2        percentage_advanced     0.024486\n",
      "12                n_snapshot     0.018087\n",
      "37                   Sandbox     0.012911\n",
      "14                1. One Box     0.008814\n",
      "25               Bear Market     0.008480\n",
      "40               Sugar Cones     0.003433\n",
      "44                   Warm Up     0.002992\n",
      "33              Orange Dance     0.002545\n",
      "21            7. Max 2 Boxes     0.002370\n",
      "26                  Bird Fez     0.002241\n",
      "15        2. Separated Boxes     0.002101\n",
      "16       3. Rotate a Pyramid     0.001966\n",
      "38     Square Cross-Sections     0.001938\n",
      "34                  Pi Henge     0.001936\n",
      "19       5. Removing Objects     0.001932\n",
      "32             Object Limits     0.001896\n",
      "20         6. Stretch a Ramp     0.001840\n",
      "24         Angled Silhouette     0.001779\n",
      "27     Boxes Obscure Spheres     0.001772\n",
      "22        8. Combine 2 Ramps     0.001667\n",
      "35      Pyramids are Strange     0.001613\n",
      "23  9. Scaling Round Objects     0.001551\n",
      "18       45-Degree Rotations     0.001380\n",
      "45                       Zzz     0.001317\n",
      "31                  Not Bird     0.001273\n",
      "36        Ramp Up and Can It     0.001228\n",
      "17      4. Match Silhouettes     0.001173\n",
      "30  More Than Meets Your Eye     0.001139\n",
      "41            Tall and Small     0.001126\n",
      "43               Unnecessary     0.001035\n",
      "39           Stranger Shapes     0.000986\n",
      "28               Bull Market     0.000846\n",
      "29                 Few Clues     0.000665\n",
      "42                 Tetromino     0.000110\n"
     ]
    }
   ],
   "source": [
    "importancia_predictores_1 = pd.DataFrame(\n",
    "    {'predictor': X_train1.columns,\n",
    "     'importancia': model_rfc_1.feature_importances_}\n",
    ")\n",
    "print(\"Importancia de los predictores en el modelo\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(importancia_predictores_1.sort_values('importancia', ascending=False))\n",
    "\n",
    "importancia_predictores_2 = pd.DataFrame(\n",
    "    {'predictor': X_train2.columns,\n",
    "     'importancia': model_rfc_2.feature_importances_}\n",
    ")\n",
    "print(\"Importancia de los predictores en el modelo\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(importancia_predictores_2.sort_values('importancia', ascending=False))\n",
    "\n",
    "importancia_predictores_3 = pd.DataFrame(\n",
    "    {'predictor': X_train3.columns,\n",
    "     'importancia': model_rfc_3.feature_importances_}\n",
    ")\n",
    "print(\"Importancia de los predictores en el modelo\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(importancia_predictores_3.sort_values('importancia', ascending=False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save model for future use"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "['./models/adaboost_3.pkl']"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_lr_1, './models/lr_1.pkl')\n",
    "joblib.dump(model_lr_2, './models/lr_2.pkl')\n",
    "joblib.dump(model_lr_3, './models/lr_3.pkl')\n",
    "\n",
    "joblib.dump(model_rfc_1, './models/rfc_1.pkl')\n",
    "joblib.dump(model_rfc_2, './models/rfc_2.pkl')\n",
    "joblib.dump(model_rfc_3, './models/rfc_3.pkl')\n",
    "\n",
    "joblib.dump(model_tree_1, './models/tree_1.pkl')\n",
    "joblib.dump(model_tree_2, './models/tree_2.pkl')\n",
    "joblib.dump(model_tree_3, './models/tree_3.pkl')\n",
    "\n",
    "joblib.dump(model_svc_1, './models/svc_1.pkl')\n",
    "joblib.dump(model_svc_2, './models/svc_2.pkl')\n",
    "joblib.dump(model_svc_3, './models/svc_3.pkl')\n",
    "\n",
    "joblib.dump(model_knn_1, './models/knn_1.pkl')\n",
    "joblib.dump(model_knn_2, './models/knn_2.pkl')\n",
    "joblib.dump(model_knn_3, './models/knn_3.pkl')\n",
    "\n",
    "joblib.dump(model_mlp_1,'./models/mlp_1.pkl')\n",
    "joblib.dump(model_mlp_2,'./models/mlp_2.pkl')\n",
    "joblib.dump(model_mlp_3,'./models/mlp_3.pkl')\n",
    "\n",
    "joblib.dump(model_sgd_1,'./models/sgd_1.pkl')\n",
    "joblib.dump(model_sgd_2,'./models/sgd_2.pkl')\n",
    "joblib.dump(model_sgd_3,'./models/sgd_3.pkl')\n",
    "\n",
    "joblib.dump(model_adaboost_1,'./models/adaboost_1.pkl')\n",
    "joblib.dump(model_adaboost_2,'./models/adaboost_2.pkl')\n",
    "joblib.dump(model_adaboost_3,'./models/adaboost_3.pkl')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "model_lr_1 = joblib.load('./models/lr_1.pkl')\n",
    "model_lr_2 = joblib.load('./models/lr_2.pkl')\n",
    "model_lr_3 = joblib.load('./models/lr_3.pkl')\n",
    "\n",
    "model_rfc_1 = joblib.load('./models/rfc_1.pkl')\n",
    "model_rfc_2 = joblib.load('./models/rfc_2.pkl')\n",
    "model_rfc_3 = joblib.load('./models/rfc_3.pkl')\n",
    "\n",
    "model_tree_1 = joblib.load('./models/tree_1.pkl')\n",
    "model_tree_2 = joblib.load('./models/tree_2.pkl')\n",
    "model_tree_3 = joblib.load('./models/tree_3.pkl')\n",
    "\n",
    "model_svc_1 = joblib.load('./models/svc_1.pkl')\n",
    "model_svc_2 = joblib.load('./models/svc_2.pkl')\n",
    "model_svc_3 = joblib.load('./models/svc_3.pkl')\n",
    "\n",
    "model_knn_1 = joblib.load('./models/knn_1.pkl')\n",
    "model_knn_2 = joblib.load('./models/knn_2.pkl')\n",
    "model_knn_3 = joblib.load('./models/knn_3.pkl')\n",
    "\n",
    "model_mlp_1 = joblib.load('./models/mlp_1.pkl')\n",
    "model_mlp_2 = joblib.load('./models/mlp_2.pkl')\n",
    "model_mlp_3 = joblib.load('./models/mlp_3.pkl')\n",
    "\n",
    "model_sgd_1 = joblib.load('./models/sgd_1.pkl')\n",
    "model_sgd_2 = joblib.load('./models/sgd_2.pkl')\n",
    "model_sgd_3 = joblib.load('./models/sgd_3.pkl')\n",
    "\n",
    "model_adaboost_1 = joblib.load('./models/adaboost_1.pkl')\n",
    "model_adaboost_2 = joblib.load('./models/adaboost_2.pkl')\n",
    "model_adaboost_3 = joblib.load('./models/adaboost_3.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
