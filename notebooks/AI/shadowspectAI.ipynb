{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Import Libraries and modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# how can i import a function to measure balanced accuracy?\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../python')\n",
    "import splitDataset\n",
    "np.set_printoptions(suppress=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data1= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil1.csv', sep=\";\")\n",
    "data2= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil2.csv', sep=\";\")\n",
    "data3= pd.read_csv('E:/Documentos\\/PCEO\\/5\\/Informatica/TFG/scripts/TFG-Informatica/Outputs/featuresOutput_percentil3.csv', sep=\";\")\n",
    "\n",
    "# want to eliminate the rows where feature bestSubmit is 100.0\n",
    "\n",
    "data1 = data1[data1.bestSubmit != 100.0]\n",
    "data2 = data2[data2.bestSubmit != 100.0]\n",
    "data3 = data3[data3.bestSubmit != 100.0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    3460\n",
      "0.0    1450\n",
      "Name: completed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts = data1['completed'].value_counts()\n",
    "\n",
    "print(counts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Split data into training and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows X_train1 dataset:  3295\n",
      "Number of rows X_test1 dataset:  1615\n",
      "Number of rows X_train2 dataset:  2551\n",
      "Number of rows X_test2 dataset:  1286\n",
      "Number of rows X_train3 dataset:  2079\n",
      "Number of rows X_test3 dataset:  971\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = splitDataset.splitDataset(data1)\n",
    "X_train2, X_test2, y_train2, y_test2 = splitDataset.splitDataset(data2)\n",
    "X_train3, X_test3, y_train3, y_test3 = splitDataset.splitDataset(data3)\n",
    "\n",
    "# show the number of rows of X_train1\n",
    "print(\"Number of rows X_train1 dataset: \", X_train1.shape[0])\n",
    "print(\"Number of rows X_test1 dataset: \", X_test1.shape[0])\n",
    "\n",
    "print(\"Number of rows X_train2 dataset: \", X_train2.shape[0])\n",
    "print(\"Number of rows X_test2 dataset: \", X_test2.shape[0])\n",
    "\n",
    "print(\"Number of rows X_train3 dataset: \", X_train3.shape[0])\n",
    "print(\"Number of rows X_test3 dataset: \", X_test3.shape[0])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Declare data preprocessing steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Declare hyperparameters to tune"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "param_grid_lr = { 'penalty' : ['l1', 'l2','elasticnet',None],\n",
    "                    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "param_grid_rfc = {'n_estimators': [150],\n",
    "                    'max_features': [5, 7, 9],\n",
    "                    'max_depth'   : [None, 3, 10, 20],\n",
    "                    'criterion'   : ['gini', 'entropy']\n",
    "                    }\n",
    "\n",
    "# another param grid for rfc but with more parameters\n",
    "param_grid_rfc2 = {'n_estimators': [150, 200, 250],\n",
    "                    'max_features': ['sqrt', 'log2', None, 5, 7, 9],\n",
    "                    'max_depth'   : [None, 3, 10, 20],\n",
    "                    'criterion'   : ['gini', 'entropy'],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 3, 5]\n",
    "                    }\n",
    "\n",
    "# i want to make a param grid for the SGD Classifier\n",
    "param_grid_sgd = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'max_iter': [100, 500, 1000],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "#param_grid_mlp = {'solver': ['lbfgs'], 'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15), 'random_state':[0,1,2,3,4,5,6,7,8,9]}\n",
    "\n",
    "param_grid_adaboost = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "param_grid_tree = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 3, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 3, 5]\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "    'p': [1, 2]\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Tune model using cross-validation pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1 Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "140 fits failed out of a total of 280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.58395281        nan 0.66477917        nan 0.62631654\n",
      "        nan 0.66477917        nan 0.66109862        nan 0.66477917\n",
      "        nan 0.66096147        nan 0.66477917        nan 0.66463854\n",
      "        nan 0.66477917        nan 0.66831822        nan 0.66477917\n",
      "        nan 0.66387814        nan 0.66477917]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [       nan 0.59112555        nan 0.67874631        nan 0.63473032\n",
      "        nan 0.67874631        nan 0.66584911        nan 0.67874631\n",
      "        nan 0.67391122        nan 0.67874631        nan 0.67768327\n",
      "        nan 0.67874631        nan 0.67943458        nan 0.67874631\n",
      "        nan 0.67827774        nan 0.67874631]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "140 fits failed out of a total of 280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.64461418        nan 0.71548393        nan 0.67672329\n",
      "        nan 0.71548393        nan 0.71569805        nan 0.71548393\n",
      "        nan 0.71571678        nan 0.71548393        nan 0.71383734\n",
      "        nan 0.71548393        nan 0.71376899        nan 0.71548393\n",
      "        nan 0.71447496        nan 0.71548393]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [       nan 0.65232841        nan 0.73157411        nan 0.6951928\n",
      "        nan 0.73157411        nan 0.72952354        nan 0.73157411\n",
      "        nan 0.72853717        nan 0.73157411        nan 0.73425555\n",
      "        nan 0.73157411        nan 0.72931367        nan 0.73157411\n",
      "        nan 0.73258325        nan 0.73157411]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "140 fits failed out of a total of 280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.71060281        nan 0.74179746        nan 0.73782536\n",
      "        nan 0.74179746        nan 0.74435944        nan 0.74179746\n",
      "        nan 0.74108501        nan 0.74179746        nan 0.75002357\n",
      "        nan 0.74179746        nan 0.73934285        nan 0.74179746\n",
      "        nan 0.74451497        nan 0.74179746]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [       nan 0.72194652        nan 0.76093823        nan 0.74790089\n",
      "        nan 0.76093823        nan 0.76017279        nan 0.76093823\n",
      "        nan 0.75959342        nan 0.76093823        nan 0.76089647\n",
      "        nan 0.76093823        nan 0.76090871        nan 0.76093823\n",
      "        nan 0.76113423        nan 0.76093823]\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "grid_lr = GridSearchCV(\n",
    "    estimator  = LogisticRegression(random_state = 33),\n",
    "    param_grid = param_grid_lr,\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "\n",
    "grid_lr_1 = grid_lr.fit(X_train1, y_train1)\n",
    "model_lr_1 = grid_lr_1.best_estimator_\n",
    "grid_lr_2 = grid_lr.fit(X_train2, y_train2)\n",
    "model_lr_2 = grid_lr_2.best_estimator_\n",
    "grid_lr_3 = grid_lr.fit(X_train3, y_train3)\n",
    "model_lr_3 = grid_lr_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2 Random Forest classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "grid_rfc = GridSearchCV(\n",
    "    estimator  = RandomForestClassifier(random_state = 123),\n",
    "    param_grid = param_grid_rfc,\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_rfc_1 = grid_rfc.fit(X = X_train1, y = y_train1)\n",
    "model_rfc_1 = grid_rfc_1.best_estimator_\n",
    "grid_rfc_2 = grid_rfc.fit(X = X_train2, y = y_train2)\n",
    "model_rfc_2 = grid_rfc_2.best_estimator_\n",
    "grid_rfc_3 = grid_rfc.fit(X = X_train3, y = y_train3)\n",
    "model_rfc_3 = grid_rfc_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo random forest para el percentil 1 es:  RandomForestClassifier(criterion='entropy', max_depth=20, max_features=5,\n",
      "                       n_estimators=150, random_state=123)\n",
      "El mejor modelo random forest para el percentil 2 es:  RandomForestClassifier(criterion='entropy', max_depth=20, max_features=5,\n",
      "                       n_estimators=150, random_state=123)\n",
      "El mejor modelo random forest para el percentil 3 es:  RandomForestClassifier(criterion='entropy', max_depth=20, max_features=5,\n",
      "                       n_estimators=150, random_state=123)\n"
     ]
    }
   ],
   "source": [
    "resultados_rfc_1 = pd.DataFrame(grid_rfc_1.cv_results_)\n",
    "resultados_rfc_1 = resultados_rfc_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_rfc_1.to_csv(\"../../Outputs/grids/grid_rfc_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo random forest para el percentil 1 es: ', grid_rfc_1.best_estimator_)\n",
    "\n",
    "resultados_rfc_2 = pd.DataFrame(grid_rfc_2.cv_results_)\n",
    "resultados_rfc_2 = resultados_rfc_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_rfc_2.to_csv(\"../../Outputs/grids/grid_rfc_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo random forest para el percentil 2 es: ', grid_rfc_2.best_estimator_)\n",
    "\n",
    "resultados_rfc_3 = pd.DataFrame(grid_rfc_3.cv_results_)\n",
    "resultados_rfc_3 = resultados_rfc_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_rfc_3.to_csv(\"../../Outputs/grids/grid_rfc_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo random forest para el percentil 3 es: ', grid_rfc_3.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 Sthocastic Gradient Descent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "grid_sgd = GridSearchCV(\n",
    "    estimator  = SGDClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_sgd,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "grid_sgd_1 = grid_sgd.fit(X = X_train1, y = y_train1)\n",
    "model_sgd_1 = grid_sgd_1.best_estimator_\n",
    "grid_sgd_2 = grid_sgd.fit(X = X_train2, y = y_train2)\n",
    "model_sgd_2 = grid_sgd_2.best_estimator_\n",
    "grid_sgd_3 = grid_sgd.fit(X = X_train3, y = y_train3)\n",
    "model_sgd_3 = grid_sgd_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo SGD para el percentil 1 es:  SGDClassifier(loss='modified_huber', max_iter=500, penalty='l1',\n",
      "              random_state=42)\n",
      "El mejor modelo SGD para el percentil 2 es:  SGDClassifier(loss='modified_huber', max_iter=500, penalty='l1',\n",
      "              random_state=42)\n",
      "El mejor modelo SGD para el percentil 3 es:  SGDClassifier(loss='modified_huber', max_iter=500, penalty='l1',\n",
      "              random_state=42)\n"
     ]
    }
   ],
   "source": [
    "resultados_sgd_1 = pd.DataFrame(grid_sgd_1.cv_results_)\n",
    "resultados_sgd_1 = resultados_sgd_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_sgd_1.to_csv(\"../../Outputs/grids/grid_sgd_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SGD para el percentil 1 es: ', grid_sgd_1.best_estimator_)\n",
    "\n",
    "resultados_sgd_2 = pd.DataFrame(grid_sgd_2.cv_results_)\n",
    "resultados_sgd_2 = resultados_sgd_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_sgd_2.to_csv(\"../../Outputs/grids/grid_sgd_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SGD para el percentil 2 es: ', grid_sgd_2.best_estimator_)\n",
    "\n",
    "resultados_sgd_3 = pd.DataFrame(grid_sgd_3.cv_results_)\n",
    "resultados_sgd_3 = resultados_sgd_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_sgd_3.to_csv(\"../../Outputs/grids/grid_sgd_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SGD para el percentil 3 es: ', grid_sgd_3.best_estimator_)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.4 MLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "grid_mlp = GridSearchCV(\n",
    "    estimator  = MLPClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_mlp,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_mlp_1 = grid_mlp.fit(X = X_train1, y = y_train1)\n",
    "model_mlp_1 = grid_mlp_1.best_estimator_\n",
    "grid_mlp_2 = grid_mlp.fit(X = X_train2, y = y_train2)\n",
    "model_mlp_2 = grid_mlp_2.best_estimator_\n",
    "grid_mlp_3 = grid_mlp.fit(X = X_train3, y = y_train3)\n",
    "model_mlp_3 = grid_mlp_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resultados_mlp_1 = pd.DataFrame(grid_mlp_1.cv_results_)\n",
    "resultados_mlp_1 = resultados_mlp_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_mlp_1.to_csv(\"../../Outputs/grids/grid_mlp_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo MLP para el percentil 1 es: ', grid_mlp_1.best_estimator_)\n",
    "\n",
    "resultados_mlp_2 = pd.DataFrame(grid_mlp_2.cv_results_)\n",
    "resultados_mlp_2 = resultados_mlp_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_mlp_2.to_csv(\"../../Outputs/grids/grid_mlp_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo MLP para el percentil 2 es: ', grid_mlp_2.best_estimator_)\n",
    "\n",
    "resultados_mlp_3 = pd.DataFrame(grid_mlp_3.cv_results_)\n",
    "resultados_mlp_3 = resultados_mlp_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_mlp_3.to_csv(\"../../Outputs/grids/grid_mlp_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo MLP para el percentil 3 es: ', grid_mlp_3.best_estimator_)\n",
    "print('Los mejores parámetros para el modelo MLP son: ', grid_mlp_3.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.5 AdaBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "grid_adaboost = GridSearchCV(\n",
    "    estimator  = AdaBoostClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_adaboost,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "\n",
    "grid_adaboost_1 = grid_adaboost.fit(X = X_train1, y = y_train1)\n",
    "model_adaboost_1 = grid_adaboost_1.best_estimator_\n",
    "grid_adaboost_2 = grid_adaboost.fit(X = X_train2, y = y_train2)\n",
    "model_adaboost_2 = grid_adaboost_2.best_estimator_\n",
    "grid_adaboost_3 = grid_adaboost.fit(X = X_train3, y = y_train3)\n",
    "model_adaboost_3 = grid_adaboost_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo AdaBoost para el percentil 1 es:  AdaBoostClassifier(learning_rate=0.1, n_estimators=500, random_state=34)\n",
      "El mejor modelo AdaBoost para el percentil 2 es:  AdaBoostClassifier(learning_rate=0.1, n_estimators=500, random_state=34)\n",
      "El mejor modelo AdaBoost para el percentil 3 es:  AdaBoostClassifier(learning_rate=0.1, n_estimators=500, random_state=34)\n",
      "Los mejores parámetros para el modelo AdaBoost son:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "resultados_adaboost_1 = pd.DataFrame(grid_adaboost_1.cv_results_)\n",
    "resultados_adaboost_1 = resultados_adaboost_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_adaboost_1.to_csv(\"../../Outputs/grids/grid_adaboost_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo AdaBoost para el percentil 1 es: ', grid_adaboost_1.best_estimator_)\n",
    "\n",
    "resultados_adaboost_2 = pd.DataFrame(grid_adaboost_2.cv_results_)\n",
    "resultados_adaboost_2 = resultados_adaboost_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_adaboost_2.to_csv(\"../../Outputs/grids/grid_adaboost_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo AdaBoost para el percentil 2 es: ', grid_adaboost_2.best_estimator_)\n",
    "\n",
    "resultados_adaboost_3 = pd.DataFrame(grid_adaboost_3.cv_results_)\n",
    "resultados_adaboost_3 = resultados_adaboost_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_adaboost_3.to_csv(\"../../Outputs/grids/grid_adaboost_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo AdaBoost para el percentil 3 es: ', grid_adaboost_3.best_estimator_)\n",
    "print('Los mejores parámetros para el modelo AdaBoost son: ', grid_adaboost_3.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.6 K-Nearest Neighbors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "grid_knn = GridSearchCV(\n",
    "    estimator  = KNeighborsClassifier(),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_knn,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_knn_1 = grid_knn.fit(X = X_train1, y = y_train1)\n",
    "model_knn_1 = grid_knn_1.best_estimator_\n",
    "grid_knn_2 = grid_knn.fit(X = X_train2, y = y_train2)\n",
    "model_knn_2 = grid_knn_2.best_estimator_\n",
    "grid_knn_3 = grid_knn.fit(X = X_train3, y = y_train3)\n",
    "model_knn_3 = grid_knn_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo KNN para el percentil 1 es:  KNeighborsClassifier(n_neighbors=9, p=1, weights='distance')\n",
      "El mejor modelo KNN para el percentil 2 es:  KNeighborsClassifier(n_neighbors=9, p=1, weights='distance')\n",
      "El mejor modelo KNN para el percentil 3 es:  KNeighborsClassifier(n_neighbors=9, p=1, weights='distance')\n",
      "Los mejores parametros para KNN para el percentil 3 es:  {'algorithm': 'auto', 'n_neighbors': 9, 'p': 1, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "resultados_knn_1 = pd.DataFrame(grid_knn_1.cv_results_)\n",
    "resultados_knn_1 = resultados_knn_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_knn_1.to_csv(\"../../Outputs/grids/grid_knn_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo KNN para el percentil 1 es: ', grid_knn_1.best_estimator_)\n",
    "\n",
    "resultados_knn_2 = pd.DataFrame(grid_knn_2.cv_results_)\n",
    "resultados_knn_2 = resultados_knn_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_knn_2.to_csv(\"../../Outputs/grids/grid_knn_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo KNN para el percentil 2 es: ', grid_knn_2.best_estimator_)\n",
    "\n",
    "resultados_knn_3 = pd.DataFrame(grid_knn_3.cv_results_)\n",
    "resultados_knn_3 = resultados_knn_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_knn_3.to_csv(\"../../Outputs/grids/grid_knn_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo KNN para el percentil 3 es: ', grid_knn_3.best_estimator_)\n",
    "print('Los mejores parametros para KNN para el percentil 3 es: ', grid_knn_3.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.7 Tree decision classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "grid_tree = GridSearchCV(\n",
    "    estimator  = DecisionTreeClassifier(random_state = 34),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_tree,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 0,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_tree_1 = grid_tree.fit(X = X_train1, y = y_train1)\n",
    "model_tree_1 = grid_tree_1.best_estimator_\n",
    "grid_tree_2 = grid_tree.fit(X = X_train2, y = y_train2)\n",
    "model_tree_2 = grid_tree_2.best_estimator_\n",
    "grid_tree_3 = grid_tree.fit(X = X_train3, y = y_train3)\n",
    "model_tree_3 = grid_tree_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor modelo Árbol de Decisión para el percentil 1 es:  DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=34)\n",
      "El mejor modelo Árbol de Decisión para el percentil 2 es:  DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=34)\n",
      "El mejor modelo Árbol de Decisión para el percentil 3 es:  DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=34)\n"
     ]
    }
   ],
   "source": [
    "resultados_tree_1 = pd.DataFrame(grid_tree_1.cv_results_)\n",
    "resultados_tree_1 = resultados_tree_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_tree_1.to_csv(\"../../Outputs/grids/grid_tree_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo Árbol de Decisión para el percentil 1 es: ', grid_tree_1.best_estimator_)\n",
    "\n",
    "resultados_tree_2 = pd.DataFrame(grid_tree_2.cv_results_)\n",
    "resultados_tree_2 = resultados_tree_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_tree_2.to_csv(\"../../Outputs/grids/grid_tree_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo Árbol de Decisión para el percentil 2 es: ', grid_tree_2.best_estimator_)\n",
    "\n",
    "resultados_tree_3 = pd.DataFrame(grid_tree_3.cv_results_)\n",
    "resultados_tree_3 = resultados_tree_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_tree_3.to_csv(\"../../Outputs/grids/grid_tree_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo Árbol de Decisión para el percentil 3 es: ', grid_tree_3.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.8 Support vector classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\anaconda3\\envs\\xAI\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    }
   ],
   "source": [
    "grid_svc = GridSearchCV(\n",
    "    estimator  = SVC(random_state = 34,max_iter=10000),\n",
    "    scoring    = 'balanced_accuracy',\n",
    "    param_grid = param_grid_svc,\n",
    "    n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "    cv         = 10,\n",
    "    refit      = True,\n",
    "    verbose    = 1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_svc_1 = grid_svc.fit(X = X_train1, y = y_train1)\n",
    "model_svc_1 = grid_svc_1.best_estimator_\n",
    "grid_svc_2 = grid_svc.fit(X = X_train2, y = y_train2)\n",
    "model_svc_2 = grid_svc_2.best_estimator_\n",
    "grid_svc_3 = grid_svc.fit(X = X_train3, y = y_train3)\n",
    "model_svc_3 = grid_svc_3.best_estimator_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resultados_svc_1 = pd.DataFrame(grid_svc_1.cv_results_)\n",
    "resultados_svc_1 = resultados_svc_1.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_svc_1.to_csv(\"../../Outputs/grids/grid_svc_1.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SVC para el percentil 1 es: ', grid_svc_1.best_estimator_)\n",
    "\n",
    "resultados_svc_2 = pd.DataFrame(grid_svc_2.cv_results_)\n",
    "resultados_svc_2 = resultados_svc_2.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_svc_2.to_csv(\"../../Outputs/grids/grid_svc_2.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SVC para el percentil 2 es: ', grid_svc_2.best_estimator_)\n",
    "\n",
    "resultados_svc_3 = pd.DataFrame(grid_svc_3.cv_results_)\n",
    "resultados_svc_3 = resultados_svc_3.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False)\n",
    "resultados_svc_3.to_csv(\"../../Outputs/grids/grid_svc_3.csv\", decimal=\".\", sep=\";\", mode='w')\n",
    "print('El mejor modelo SVC para el percentil 3 es: ', grid_svc_3.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Refit on the entire training set\n",
    "# No additional code needed if clf.refit == True (default is True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate model pipeline on test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "y_pred_lr_1 = model_lr_1.predict(X_test1)\n",
    "y_pred_binary_lr_1 = np.where(y_pred_lr_1 > 0.5, 1, 0)\n",
    "\n",
    "y_pred_lr_2 = model_lr_2.predict(X_test2)\n",
    "y_pred_binary_lr_2 = np.where(y_pred_lr_2 > 0.5, 1, 0)\n",
    "\n",
    "y_pred_lr_3 = model_lr_3.predict(X_test3)\n",
    "y_pred_binary_lr_3 = np.where(y_pred_lr_3 > 0.5, 1, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using lr is : 0.6842594963721724\n",
      "The balanced accuracy for percentile 2 using lr is : 0.7256444313262496\n",
      "The balanced accuracy for percentile 3 using lr is : 0.7432330380477208\n",
      "The f1 score for percentile 1 using lr is : 0.8361629881154499\n",
      "The f1 score for percentile 2 using lr is : 0.7932118164676304\n",
      "The f1 score for percentile 3 using lr is : 0.7598842815814851\n",
      "The cohen kappa score for percentile 1 using lr is : 0.40917060780392567\n",
      "The cohen kappa score for percentile 2 using lr is : 0.4644691574920954\n",
      "The cohen kappa score for percentile 3 using lr is : 0.48678102240911225\n",
      "The confusion matrix for percentile 1 using lr is : [[244 306]\n",
      " [ 80 985]]\n",
      "The confusion matrix for percentile 2 using lr is : [[326 234]\n",
      " [ 95 631]]\n",
      "The confusion matrix for percentile 3 using lr is : [[328 155]\n",
      " [ 94 394]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using lr is : ' + str(balanced_accuracy_score(y_test1, y_pred_binary_lr_1)))\n",
    "print('The balanced accuracy for percentile 2 using lr is : ' + str(balanced_accuracy_score(y_test2, y_pred_binary_lr_2)))\n",
    "print('The balanced accuracy for percentile 3 using lr is : ' + str(balanced_accuracy_score(y_test3, y_pred_binary_lr_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using lr is : ' + str(f1_score(y_test1, y_pred_binary_lr_1)))\n",
    "print('The f1 score for percentile 2 using lr is : ' + str(f1_score(y_test2, y_pred_binary_lr_2)))\n",
    "print('The f1 score for percentile 3 using lr is : ' + str(f1_score(y_test3, y_pred_binary_lr_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using lr is : ' + str(cohen_kappa_score(y_test1, y_pred_binary_lr_1)))\n",
    "print('The cohen kappa score for percentile 2 using lr is : ' + str(cohen_kappa_score(y_test2, y_pred_binary_lr_2)))\n",
    "print('The cohen kappa score for percentile 3 using lr is : ' + str(cohen_kappa_score(y_test3, y_pred_binary_lr_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using lr is : ' + str(confusion_matrix(y_test1, y_pred_binary_lr_1)))\n",
    "print('The confusion matrix for percentile 2 using lr is : ' + str(confusion_matrix(y_test2, y_pred_binary_lr_2)))\n",
    "print('The confusion matrix for percentile 3 using lr is : ' + str(confusion_matrix(y_test3, y_pred_binary_lr_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "y_pred_rfc_1 = model_rfc_1.predict(X_test1)\n",
    "y_pred_rfc_2 = model_rfc_2.predict(X_test2)\n",
    "y_pred_rfc_3 = model_rfc_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using rfc is : 0.7536491677336747\n",
      "The balanced accuracy for percentile 2 using rfc is : 0.7839334907516726\n",
      "The balanced accuracy for percentile 3 using rfc is : 0.7978226928690222\n",
      "The f1 score for percentile 1 using rfc is : 0.8701298701298701\n",
      "The f1 score for percentile 2 using rfc is : 0.8356687898089172\n",
      "The f1 score for percentile 3 using rfc is : 0.8108108108108109\n",
      "The cohen kappa score for percentile 1 using rfc is : 0.5508898776418243\n",
      "The cohen kappa score for percentile 2 using rfc is : 0.5818845364774199\n",
      "The cohen kappa score for percentile 3 using rfc is : 0.5960246907290897\n",
      "The confusion matrix for percentile 1 using rfc is : [[ 310  240]\n",
      " [  60 1005]]\n",
      "The confusion matrix for percentile 2 using rfc is : [[372 188]\n",
      " [ 70 656]]\n",
      "The confusion matrix for percentile 3 using rfc is : [[355 128]\n",
      " [ 68 420]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using rfc is : ' + str(balanced_accuracy_score(y_test1, y_pred_rfc_1)))\n",
    "print('The balanced accuracy for percentile 2 using rfc is : ' + str(balanced_accuracy_score(y_test2, y_pred_rfc_2)))\n",
    "print('The balanced accuracy for percentile 3 using rfc is : ' + str(balanced_accuracy_score(y_test3, y_pred_rfc_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using rfc is : ' + str(f1_score(y_test1, y_pred_rfc_1)))\n",
    "print('The f1 score for percentile 2 using rfc is : ' + str(f1_score(y_test2, y_pred_rfc_2)))\n",
    "print('The f1 score for percentile 3 using rfc is : ' + str(f1_score(y_test3, y_pred_rfc_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using rfc is : ' + str(cohen_kappa_score(y_test1, y_pred_rfc_1)))\n",
    "print('The cohen kappa score for percentile 2 using rfc is : ' + str(cohen_kappa_score(y_test2, y_pred_rfc_2)))\n",
    "print('The cohen kappa score for percentile 3 using rfc is : ' + str(cohen_kappa_score(y_test3, y_pred_rfc_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using rfc is : ' + str(confusion_matrix(y_test1, y_pred_rfc_1)))\n",
    "print('The confusion matrix for percentile 2 using rfc is : ' + str(confusion_matrix(y_test2, y_pred_rfc_2)))\n",
    "print('The confusion matrix for percentile 3 using rfc is : ' + str(confusion_matrix(y_test3, y_pred_rfc_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "y_pred_sgd_1 = model_sgd_1.predict(X_test1)\n",
    "y_pred_sgd_2 = model_sgd_2.predict(X_test2)\n",
    "y_pred_sgd_3 = model_sgd_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using sgd is : 0.7241912078531797\n",
      "The balanced accuracy for percentile 2 using sgd is : 0.7272087760724124\n",
      "The balanced accuracy for percentile 3 using sgd is : 0.7140460068560568\n",
      "The f1 score for percentile 1 using sgd is : 0.84108012394865\n",
      "The f1 score for percentile 2 using sgd is : 0.7438494934876989\n",
      "The f1 score for percentile 3 using sgd is : 0.7488667271078876\n",
      "The cohen kappa score for percentile 1 using sgd is : 0.47533855474564846\n",
      "The cohen kappa score for percentile 2 using sgd is : 0.44801513204161314\n",
      "The cohen kappa score for percentile 3 using sgd is : 0.4286694436596832\n",
      "The confusion matrix for percentile 1 using sgd is : [[306 244]\n",
      " [115 950]]\n",
      "The confusion matrix for percentile 2 using sgd is : [[418 142]\n",
      " [212 514]]\n",
      "The confusion matrix for percentile 3 using sgd is : [[281 202]\n",
      " [ 75 413]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using sgd is : ' + str(balanced_accuracy_score(y_test1, y_pred_sgd_1)))\n",
    "print('The balanced accuracy for percentile 2 using sgd is : ' + str(balanced_accuracy_score(y_test2, y_pred_sgd_2)))\n",
    "print('The balanced accuracy for percentile 3 using sgd is : ' + str(balanced_accuracy_score(y_test3, y_pred_sgd_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using sgd is : ' + str(f1_score(y_test1, y_pred_sgd_1)))\n",
    "print('The f1 score for percentile 2 using sgd is : ' + str(f1_score(y_test2, y_pred_sgd_2)))\n",
    "print('The f1 score for percentile 3 using sgd is : ' + str(f1_score(y_test3, y_pred_sgd_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using sgd is : ' + str(cohen_kappa_score(y_test1, y_pred_sgd_1)))\n",
    "print('The cohen kappa score for percentile 2 using sgd is : ' + str(cohen_kappa_score(y_test2, y_pred_sgd_2)))\n",
    "print('The cohen kappa score for percentile 3 using sgd is : ' + str(cohen_kappa_score(y_test3, y_pred_sgd_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using sgd is : ' + str(confusion_matrix(y_test1, y_pred_sgd_1)))\n",
    "print('The confusion matrix for percentile 2 using sgd is : ' + str(confusion_matrix(y_test2, y_pred_sgd_2)))\n",
    "print('The confusion matrix for percentile 3 using sgd is : ' + str(confusion_matrix(y_test3, y_pred_sgd_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "y_pred_mlp_1 = model_mlp_1.predict(X_test1)\n",
    "y_pred_mlp_2 = model_mlp_2.predict(X_test2)\n",
    "y_pred_mlp_3 = model_mlp_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using mlp is : 0.7420614596670935\n",
      "The balanced accuracy for percentile 2 using mlp is : 0.7785050177095632\n",
      "The balanced accuracy for percentile 3 using mlp is : 0.7976105624002987\n",
      "The f1 score for percentile 1 using mlp is : 0.8608695652173913\n",
      "The f1 score for percentile 2 using mlp is : 0.8055363321799308\n",
      "The f1 score for percentile 3 using mlp is : 0.8178438661710038\n",
      "The cohen kappa score for percentile 1 using mlp is : 0.523225241016652\n",
      "The cohen kappa score for percentile 2 using mlp is : 0.5562151687007695\n",
      "The cohen kappa score for percentile 3 using mlp is : 0.5958531178064506\n",
      "The confusion matrix for percentile 1 using mlp is : [[305 245]\n",
      " [ 75 990]]\n",
      "The confusion matrix for percentile 2 using mlp is : [[423 137]\n",
      " [144 582]]\n",
      "The confusion matrix for percentile 3 using mlp is : [[335 148]\n",
      " [ 48 440]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using mlp is : ' + str(balanced_accuracy_score(y_test1, y_pred_mlp_1)))\n",
    "print('The balanced accuracy for percentile 2 using mlp is : ' + str(balanced_accuracy_score(y_test2, y_pred_mlp_2)))\n",
    "print('The balanced accuracy for percentile 3 using mlp is : ' + str(balanced_accuracy_score(y_test3, y_pred_mlp_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using mlp is : ' + str(f1_score(y_test1, y_pred_mlp_1)))\n",
    "print('The f1 score for percentile 2 using mlp is : ' + str(f1_score(y_test2, y_pred_mlp_2)))\n",
    "print('The f1 score for percentile 3 using mlp is : ' + str(f1_score(y_test3, y_pred_mlp_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using mlp is : ' + str(cohen_kappa_score(y_test1, y_pred_mlp_1)))\n",
    "print('The cohen kappa score for percentile 2 using mlp is : ' + str(cohen_kappa_score(y_test2, y_pred_mlp_2)))\n",
    "print('The cohen kappa score for percentile 3 using mlp is : ' + str(cohen_kappa_score(y_test3, y_pred_mlp_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using mlp is : ' + str(confusion_matrix(y_test1, y_pred_mlp_1)))\n",
    "print('The confusion matrix for percentile 2 using mlp is : ' + str(confusion_matrix(y_test2, y_pred_mlp_2)))\n",
    "print('The confusion matrix for percentile 3 using mlp is : ' + str(confusion_matrix(y_test3, y_pred_mlp_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "y_pred_adaboost_1 = model_adaboost_1.predict(X_test1)\n",
    "y_pred_adaboost_2 = model_adaboost_2.predict(X_test2)\n",
    "y_pred_adaboost_3 = model_adaboost_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR ADABOOST\n",
      "\n",
      "-----Balanced accuracy-----\n",
      "percentile 1 : 0.7697737942808365\n",
      "percentile 2 : 0.7764339826839827\n",
      "percentile 3 : 0.8010343481654957\n",
      "\n",
      "-----F1 score-----\n",
      "percentile 1 : 0.8721340388007055\n",
      "percentile 2 : 0.8288633461047253\n",
      "percentile 3 : 0.8094768015794668\n",
      "\n",
      "-----Cohen kappa score-----\n",
      "percentile 1 : 0.5743936461201531\n",
      "percentile 2 : 0.5660416320404535\n",
      "percentile 3 : 0.6023050657544304\n",
      "\n",
      "-----Confusion matrix-----\n",
      "percentile 1 : [[336 214]\n",
      " [ 76 989]]\n",
      "percentile 2 : [[369 191]\n",
      " [ 77 649]]\n",
      "percentile 3 : [[368 115]\n",
      " [ 78 410]]\n"
     ]
    }
   ],
   "source": [
    "print('RESULTS FOR ADABOOST')\n",
    "print()\n",
    "print('-----Balanced accuracy-----')\n",
    "print('percentile 1 : ' + str(balanced_accuracy_score(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(balanced_accuracy_score(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(balanced_accuracy_score(y_test3, y_pred_adaboost_3)))\n",
    "print()\n",
    "print('-----F1 score-----')\n",
    "print('percentile 1 : ' + str(f1_score(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(f1_score(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(f1_score(y_test3, y_pred_adaboost_3)))\n",
    "print()\n",
    "print('-----Cohen kappa score-----')\n",
    "print('percentile 1 : ' + str(cohen_kappa_score(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(cohen_kappa_score(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(cohen_kappa_score(y_test3, y_pred_adaboost_3)))\n",
    "print()\n",
    "print('-----Confusion matrix-----')\n",
    "print('percentile 1 : ' + str(confusion_matrix(y_test1, y_pred_adaboost_1)))\n",
    "print('percentile 2 : ' + str(confusion_matrix(y_test2, y_pred_adaboost_2)))\n",
    "print('percentile 3 : ' + str(confusion_matrix(y_test3, y_pred_adaboost_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "y_pred_knn_1 = model_knn_1.predict(X_test1)\n",
    "y_pred_knn_2 = model_knn_2.predict(X_test2)\n",
    "y_pred_knn_3 = model_knn_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using knn is : 0.6754588134869824\n",
      "The balanced accuracy for percentile 2 using knn is : 0.6930416174734357\n",
      "The balanced accuracy for percentile 3 using knn is : 0.7409823337745647\n",
      "The f1 score for percentile 1 using knn is : 0.8292890591741165\n",
      "The f1 score for percentile 2 using knn is : 0.7795992714025501\n",
      "The f1 score for percentile 3 using knn is : 0.766076421248835\n",
      "The cohen kappa score for percentile 1 using knn is : 0.3883009119545864\n",
      "The cohen kappa score for percentile 2 using knn is : 0.402090297790586\n",
      "The cohen kappa score for percentile 3 using knn is : 0.4824610392781835\n",
      "The confusion matrix for percentile 1 using knn is : [[240 310]\n",
      " [ 91 974]]\n",
      "The confusion matrix for percentile 2 using knn is : [[281 279]\n",
      " [ 84 642]]\n",
      "The confusion matrix for percentile 3 using knn is : [[309 174]\n",
      " [ 77 411]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using knn is : ' + str(balanced_accuracy_score(y_test1, y_pred_knn_1)))\n",
    "print('The balanced accuracy for percentile 2 using knn is : ' + str(balanced_accuracy_score(y_test2, y_pred_knn_2)))\n",
    "print('The balanced accuracy for percentile 3 using knn is : ' + str(balanced_accuracy_score(y_test3, y_pred_knn_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using knn is : ' + str(f1_score(y_test1, y_pred_knn_1)))\n",
    "print('The f1 score for percentile 2 using knn is : ' + str(f1_score(y_test2, y_pred_knn_2)))\n",
    "print('The f1 score for percentile 3 using knn is : ' + str(f1_score(y_test3, y_pred_knn_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using knn is : ' + str(cohen_kappa_score(y_test1, y_pred_knn_1)))\n",
    "print('The cohen kappa score for percentile 2 using knn is : ' + str(cohen_kappa_score(y_test2, y_pred_knn_2)))\n",
    "print('The cohen kappa score for percentile 3 using knn is : ' + str(cohen_kappa_score(y_test3, y_pred_knn_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using knn is : ' + str(confusion_matrix(y_test1, y_pred_knn_1)))\n",
    "print('The confusion matrix for percentile 2 using knn is : ' + str(confusion_matrix(y_test2, y_pred_knn_2)))\n",
    "print('The confusion matrix for percentile 3 using knn is : ' + str(confusion_matrix(y_test3, y_pred_knn_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "y_pred_tree_1 = model_tree_1.predict(X_test1)\n",
    "y_pred_tree_2 = model_tree_2.predict(X_test2)\n",
    "y_pred_tree_3 = model_tree_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using tree is : 0.724950917626974\n",
      "The balanced accuracy for percentile 2 using tree is : 0.7526834907516726\n",
      "The balanced accuracy for percentile 3 using tree is : 0.7608525947798934\n",
      "The f1 score for percentile 1 using tree is : 0.8381631743201069\n",
      "The f1 score for percentile 2 using tree is : 0.8174454828660436\n",
      "The f1 score for percentile 3 using tree is : 0.7716535433070867\n",
      "The cohen kappa score for percentile 1 using tree is : 0.473419233723014\n",
      "The cohen kappa score for percentile 2 using tree is : 0.5216611951932281\n",
      "The cohen kappa score for percentile 3 using tree is : 0.5219266226379858\n",
      "The confusion matrix for percentile 1 using tree is : [[312 238]\n",
      " [125 940]]\n",
      "The confusion matrix for percentile 2 using tree is : [[337 223]\n",
      " [ 70 656]]\n",
      "The confusion matrix for percentile 3 using tree is : [[347 136]\n",
      " [ 96 392]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using tree is : ' + str(balanced_accuracy_score(y_test1, y_pred_tree_1)))\n",
    "print('The balanced accuracy for percentile 2 using tree is : ' + str(balanced_accuracy_score(y_test2, y_pred_tree_2)))\n",
    "print('The balanced accuracy for percentile 3 using tree is : ' + str(balanced_accuracy_score(y_test3, y_pred_tree_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using tree is : ' + str(f1_score(y_test1, y_pred_tree_1)))\n",
    "print('The f1 score for percentile 2 using tree is : ' + str(f1_score(y_test2, y_pred_tree_2)))\n",
    "print('The f1 score for percentile 3 using tree is : ' + str(f1_score(y_test3, y_pred_tree_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using tree is : ' + str(cohen_kappa_score(y_test1, y_pred_tree_1)))\n",
    "print('The cohen kappa score for percentile 2 using tree is : ' + str(cohen_kappa_score(y_test2, y_pred_tree_2)))\n",
    "print('The cohen kappa score for percentile 3 using tree is : ' + str(cohen_kappa_score(y_test3, y_pred_tree_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using tree is : ' + str(confusion_matrix(y_test1, y_pred_tree_1)))\n",
    "print('The confusion matrix for percentile 2 using tree is : ' + str(confusion_matrix(y_test2, y_pred_tree_2)))\n",
    "print('The confusion matrix for percentile 3 using tree is : ' + str(confusion_matrix(y_test3, y_pred_tree_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "y_pred_svc_1 = model_svc_1.predict(X_test1)\n",
    "y_pred_svc_2 = model_svc_2.predict(X_test2)\n",
    "y_pred_svc_3 = model_svc_3.predict(X_test3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy for percentile 1 using svc is : 0.7015023474178403\n",
      "The balanced accuracy for percentile 2 using svc is : 0.7312303227075954\n",
      "The balanced accuracy for percentile 3 using svc is : 0.7451019074771748\n",
      "The f1 score for percentile 1 using svc is : 0.842330762639246\n",
      "The f1 score for percentile 2 using svc is : 0.800747198007472\n",
      "The f1 score for percentile 3 using svc is : 0.7698042870456664\n",
      "The cohen kappa score for percentile 1 using svc is : 0.4427065751472188\n",
      "The cohen kappa score for percentile 2 using svc is : 0.4774720781760281\n",
      "The cohen kappa score for percentile 3 using svc is : 0.4907086721183719\n",
      "The confusion matrix for percentile 1 using svc is : [[264 286]\n",
      " [ 82 983]]\n",
      "The confusion matrix for percentile 2 using svc is : [[323 237]\n",
      " [ 83 643]]\n",
      "The confusion matrix for percentile 3 using svc is : [[311 172]\n",
      " [ 75 413]]\n"
     ]
    }
   ],
   "source": [
    "print('The balanced accuracy for percentile 1 using svc is : ' + str(balanced_accuracy_score(y_test1, y_pred_svc_1)))\n",
    "print('The balanced accuracy for percentile 2 using svc is : ' + str(balanced_accuracy_score(y_test2, y_pred_svc_2)))\n",
    "print('The balanced accuracy for percentile 3 using svc is : ' + str(balanced_accuracy_score(y_test3, y_pred_svc_3)))\n",
    "\n",
    "print('The f1 score for percentile 1 using svc is : ' + str(f1_score(y_test1, y_pred_svc_1)))\n",
    "print('The f1 score for percentile 2 using svc is : ' + str(f1_score(y_test2, y_pred_svc_2)))\n",
    "print('The f1 score for percentile 3 using svc is : ' + str(f1_score(y_test3, y_pred_svc_3)))\n",
    "\n",
    "print('The cohen kappa score for percentile 1 using svc is : ' + str(cohen_kappa_score(y_test1, y_pred_svc_1)))\n",
    "print('The cohen kappa score for percentile 2 using svc is : ' + str(cohen_kappa_score(y_test2, y_pred_svc_2)))\n",
    "print('The cohen kappa score for percentile 3 using svc is : ' + str(cohen_kappa_score(y_test3, y_pred_svc_3)))\n",
    "\n",
    "print('The confusion matrix for percentile 1 using svc is : ' + str(confusion_matrix(y_test1, y_pred_svc_1)))\n",
    "print('The confusion matrix for percentile 2 using svc is : ' + str(confusion_matrix(y_test2, y_pred_svc_2)))\n",
    "print('The confusion matrix for percentile 3 using svc is : ' + str(confusion_matrix(y_test3, y_pred_svc_3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importancia de los predictores\n",
    "\n",
    "##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importancia_predictores_1 = pd.DataFrame(\n",
    "    {'predictor': X_train1.columns,\n",
    "     'importancia': model_rfc_1.feature_importances_}\n",
    ")\n",
    "print(\"Importancia de los predictores en el modelo\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(importancia_predictores_1.sort_values('importancia', ascending=False))\n",
    "\n",
    "importancia_predictores_2 = pd.DataFrame(\n",
    "    {'predictor': X_train2.columns,\n",
    "     'importancia': model_rfc_2.feature_importances_}\n",
    ")\n",
    "print(\"Importancia de los predictores en el modelo\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(importancia_predictores_2.sort_values('importancia', ascending=False))\n",
    "\n",
    "importancia_predictores_3 = pd.DataFrame(\n",
    "    {'predictor': X_train3.columns,\n",
    "     'importancia': model_rfc_3.feature_importances_}\n",
    ")\n",
    "print(\"Importancia de los predictores en el modelo\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(importancia_predictores_3.sort_values('importancia', ascending=False))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save model for future use"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['./models2/mlp_3.pkl']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_lr_1, './models2/lr_1.pkl')\n",
    "joblib.dump(model_lr_2, './models2/lr_2.pkl')\n",
    "joblib.dump(model_lr_3, './models2/lr_3.pkl')\n",
    "\n",
    "joblib.dump(model_rfc_1, './models2/rfc_1.pkl')\n",
    "joblib.dump(model_rfc_2, './models2/rfc_2.pkl')\n",
    "joblib.dump(model_rfc_3, './models2/rfc_3.pkl')\n",
    "\n",
    "joblib.dump(model_tree_1, './models2/tree_1.pkl')\n",
    "joblib.dump(model_tree_2, './models2/tree_2.pkl')\n",
    "joblib.dump(model_tree_3, './models2/tree_3.pkl')\n",
    "\n",
    "joblib.dump(model_svc_1, './models2/svc_1.pkl')\n",
    "joblib.dump(model_svc_2, './models2/svc_2.pkl')\n",
    "joblib.dump(model_svc_3, './models2/svc_3.pkl')\n",
    "\n",
    "joblib.dump(model_knn_1, './models2/knn_1.pkl')\n",
    "joblib.dump(model_knn_2, './models2/knn_2.pkl')\n",
    "joblib.dump(model_knn_3, './models2/knn_3.pkl')\n",
    "\n",
    "joblib.dump(model_mlp_1,'./models2/mlp_1.pkl')\n",
    "joblib.dump(model_mlp_2,'./models2/mlp_2.pkl')\n",
    "joblib.dump(model_mlp_3,'./models2/mlp_3.pkl')\n",
    "\n",
    "joblib.dump(model_sgd_1,'./models2/sgd_1.pkl')\n",
    "joblib.dump(model_sgd_2,'./models2/sgd_2.pkl')\n",
    "joblib.dump(model_sgd_3,'./models2/sgd_3.pkl')\n",
    "\n",
    "joblib.dump(model_adaboost_1,'./models2/adaboost_1.pkl')\n",
    "joblib.dump(model_adaboost_2,'./models2/adaboost_2.pkl')\n",
    "joblib.dump(model_adaboost_3,'./models2/adaboost_3.pkl')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_lr_1 = joblib.load('./models/lr_1.pkl')\n",
    "model_lr_2 = joblib.load('./models/lr_2.pkl')\n",
    "model_lr_3 = joblib.load('./models/lr_3.pkl')\n",
    "\n",
    "model_rfc_1 = joblib.load('./models/rfc_1.pkl')\n",
    "model_rfc_2 = joblib.load('./models/rfc_2.pkl')\n",
    "model_rfc_3 = joblib.load('./models/rfc_3.pkl')\n",
    "\n",
    "model_tree_1 = joblib.load('./models/tree_1.pkl')\n",
    "model_tree_2 = joblib.load('./models/tree_2.pkl')\n",
    "model_tree_3 = joblib.load('./models/tree_3.pkl')\n",
    "\n",
    "model_svc_1 = joblib.load('./models/svc_1.pkl')\n",
    "model_svc_2 = joblib.load('./models/svc_2.pkl')\n",
    "model_svc_3 = joblib.load('./models/svc_3.pkl')\n",
    "\n",
    "model_knn_1 = joblib.load('./models/knn_1.pkl')\n",
    "model_knn_2 = joblib.load('./models/knn_2.pkl')\n",
    "model_knn_3 = joblib.load('./models/knn_3.pkl')\n",
    "\n",
    "model_mlp_1 = joblib.load('./models/mlp_1.pkl')\n",
    "model_mlp_2 = joblib.load('./models/mlp_2.pkl')\n",
    "model_mlp_3 = joblib.load('./models/mlp_3.pkl')\n",
    "\n",
    "model_sgd_1 = joblib.load('./models/sgd_1.pkl')\n",
    "model_sgd_2 = joblib.load('./models/sgd_2.pkl')\n",
    "model_sgd_3 = joblib.load('./models/sgd_3.pkl')\n",
    "\n",
    "model_adaboost_1 = joblib.load('./models/adaboost_1.pkl')\n",
    "model_adaboost_2 = joblib.load('./models/adaboost_2.pkl')\n",
    "model_adaboost_3 = joblib.load('./models/adaboost_3.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
